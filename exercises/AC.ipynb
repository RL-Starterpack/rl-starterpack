{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LojKEAuSWYEF"
   },
   "source": [
    "# RL Tutorial - **AC Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Kx5mOxWYEF"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "OMSPBHiMWYEF"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to clone the RL tutorial repository and install it\n",
    "try:\n",
    "  import rl_starterpack\n",
    "  print('RL-Starterpack repo succesfully installed!')\n",
    "except ImportError:\n",
    "  print('Cloning RL-Starterpack package...')\n",
    "\n",
    "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
    "  print('Installing RL-StarterPack package...')\n",
    "  !pip install -e rl-starterpack[full] &> /dev/null\n",
    "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "  print('Please restart the runtime to use the newly installed package!')\n",
    "  print('Runtime > Restart Runtime')\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "-MD_29DnWYEF"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to install additional dependencies (will take ~30s)\n",
    "!apt-get remove ffmpeg > /dev/null # Removing due to restrictive license\n",
    "!apt-get install -y xvfb python-opengl > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "Uf9vBEm-WYEF"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to import the required libraries\n",
    "try:\n",
    "    from rl_starterpack import AC, OpenAIGym, PG, experiment, vis_utils\n",
    "except ImportError:\n",
    "    print('Please run the first cell! If you already ran it, make sure '\n",
    "          'to restart the runtime after the package is installed.')\n",
    "    raise\n",
    "\n",
    "import gym\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchviz\n",
    "from tqdm.auto import tqdm\n",
    "%matplotlib inline\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Setup display to show video renderings\n",
    "if 'display' not in globals():\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ocFWB19gWYEF"
   },
   "source": [
    "You've seen most of these steps before, so we'll quickly go through them here, focusing on various actor-critic configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I53seRl-WYEF"
   },
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hpqMV2yyWYEF"
   },
   "source": [
    "Setup CartPole environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W3yBe6X3WYEF"
   },
   "outputs": [],
   "source": [
    "env = OpenAIGym(level='CartPole', max_timesteps=300)\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x50NS6PWYEF"
   },
   "source": [
    "Actor and critic network constructors (only difference: output size as #actions vs 1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqo9ATHIWYEF"
   },
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "\n",
    "def actor_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values'])\n",
    "    )\n",
    "\n",
    "def critic_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KYMQpdUXWYEF"
   },
   "source": [
    "### Q actor-critic using estimated returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmQ9Y1DmWYEF"
   },
   "source": [
    "Agent configuration, without any optional extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3_hQE3fnWYEF"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlGkGLO_WYEF"
   },
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXi-KgiiWYEF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for n in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0YigzLoWYEF"
   },
   "source": [
    "Plot of episode returns: occasionally successful, but generally not stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cud2hClTWYEF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_qimM8oWYEF"
   },
   "source": [
    "Plot of actor loss: consistently relatively high, due to being weighted by cumulative return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y1bxX_GjWYEF",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tv-YA_Q3WYEF"
   },
   "source": [
    "Plot of critic loss: converging, occasionally increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWTtY4sjWYEF",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJrOtIPXWYEF"
   },
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a7zq4pK3WYEF"
   },
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uFTKsQnTWYEF"
   },
   "source": [
    "### Q actor-critic using normalized estimated returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ldKyNxNPWYEF"
   },
   "source": [
    "Agent configuration, this time with `normalize_returns` set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lp6EynxjWYEF"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J2T1IGqcWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5grLk24WYEG"
   },
   "source": [
    "Plot of episode returns: more consistently improving, still unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb9GRM7hWYEG",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XbYUZvWEWYEG"
   },
   "source": [
    "Plot of actor loss: consistently around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pef3KWl4WYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oTNgUlbdWYEG"
   },
   "source": [
    "Plot of critic loss: converging to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SKsJG5KQWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8GznXuuWYEG"
   },
   "source": [
    "It is interesting to compare this to the performance of our policy-gradient algorithm with normalized returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OO0_0tuHWYEG"
   },
   "outputs": [],
   "source": [
    "agent = PG(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    network_fn=actor_fn, learning_rate=1e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "returns = list()\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})\n",
    "\n",
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q2LQiB6YWYEG"
   },
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMYFo8UFWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDC8W37tWYEG"
   },
   "source": [
    "### TD actor-critic using estimated advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Km_fs3vQWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, compute_advantage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OG2macFAWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HPngpMeWYEG"
   },
   "source": [
    "Plot of episode returns: improving, occasionally unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GVfehQ8MWYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hw4XY8NzWYEG"
   },
   "source": [
    "Plot of actor loss: converging to zero very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-xrBp_L8WYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqPvQIsoWYEG"
   },
   "source": [
    "Plot of critic loss: converging to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uHB8mbC4WYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Sz65I40WYEG"
   },
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vwpiu2D7WYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho4A_4GvWYEG"
   },
   "source": [
    "## FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have time, feel free to play around with FrozenLake below. Note that the hyperparameters as currently configured will not solve the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2cGeUkQWYEG"
   },
   "outputs": [],
   "source": [
    "# reward_threshold: 0.78, optimum: 0.8196, max_timesteps: 100\n",
    "env = OpenAIGym(level='FrozenLake', max_timesteps=100)\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yzVxhe3gWYEG"
   },
   "outputs": [],
   "source": [
    "def reward_shaping_fn(reward, terminal, state):\n",
    "    if terminal == 1 and reward == 0.0:\n",
    "        return -1.0, terminal\n",
    "    elif terminal == 2 and reward == 0.0:\n",
    "        return -0.5, terminal\n",
    "    else:\n",
    "        return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ilPbmoWYEG"
   },
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "\n",
    "def actor_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values']),\n",
    "    )\n",
    "\n",
    "def critic_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo-ltBHlWYEG"
   },
   "source": [
    "### Q actor-critic using estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kaq7S2clWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUhJPSBHWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upmXMK_lWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0ysvMszWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPUKFhO-WYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In_pafnbWYEG"
   },
   "source": [
    "### Q actor-critic using normalized estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PDxPHzhWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jECOmNwiWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-fy8rGiWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXVQkh0xWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzWdNfOyWYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_neiY4JzWYEG"
   },
   "source": [
    "### TD actor-critic using estimated advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GKIP7uyWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, compute_advantage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyaGMqG7WYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zXJT0SZWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j95ntfyUWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NljAfLbsWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ho4A_4GvWYEG"
   },
   "source": [
    "## Pendulum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e2cGeUkQWYEG"
   },
   "outputs": [],
   "source": [
    "env = OpenAIGym(level='Pendulum', max_timesteps=100)\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "43ilPbmoWYEG"
   },
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "\n",
    "def actor_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['shape'][0])\n",
    "    )\n",
    "\n",
    "def critic_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo-ltBHlWYEG"
   },
   "source": [
    "### Q actor-critic using estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kaq7S2clWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kUhJPSBHWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "upmXMK_lWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0ysvMszWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UPUKFhO-WYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "In_pafnbWYEG"
   },
   "source": [
    "### Q actor-critic using normalized estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3PDxPHzhWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jECOmNwiWYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E-fy8rGiWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jXVQkh0xWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wzWdNfOyWYEG",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_neiY4JzWYEG"
   },
   "source": [
    "### TD actor-critic using estimated advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1GKIP7uyWYEG"
   },
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, compute_advantage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JyaGMqG7WYEG"
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7zXJT0SZWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j95ntfyUWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NljAfLbsWYEG"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "AC.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
