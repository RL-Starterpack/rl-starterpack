{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Tutorial - **AC Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to clone the RL tutorial repository and install it\n",
    "import os\n",
    "from getpass import getpass\n",
    "import urllib\n",
    "\n",
    "try:\n",
    "  import rl_starterpack\n",
    "  print('RL-Starterpack repo succesfully installed!')\n",
    "except ImportError:\n",
    "  print('Cloning RL-StarterPack package...')\n",
    "  user = input('User name: ')\n",
    "  password = getpass('Password: ')\n",
    "  password = urllib.parse.quote(password) # your password is converted into url format\n",
    "\n",
    "  !git clone https://{user}:{password}@github.com/AlexKuhnle/rl-starterpack.git\n",
    "  print('Installing RL-StarterPack package...')\n",
    "  !pip install -e rl-starterpack &> /dev/null\n",
    "  del password\n",
    "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "  print('Please restart the runtime to use the newly installed package!')\n",
    "  print('Runtime > Restart Runtime')\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to install additional dependencies (will take ~30s)\n",
    "!pip install torchviz > /dev/null\n",
    "!pip install gym pyvirtualdisplay > /dev/null\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to import the required libraries\n",
    "try:\n",
    "    from rl_starterpack import AC, OpenAIGym, PG, experiment, vis_utils\n",
    "except ImportError:\n",
    "    print('Please run the first cell! If you already ran it, make sure '\n",
    "          'to restart the runtime after the package is installed.')\n",
    "    raise\n",
    "\n",
    "import gym\n",
    "from gym import logger as gymlogger\n",
    "from IPython import display as ipythondisplay\n",
    "from itertools import chain\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchviz\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "gymlogger.set_level(40) #error only\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You've seen most of these steps before, so we'll quickly go through them here, focusing on various actor-critic configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CartPole"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup CartPole environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = OpenAIGym(level='CartPole', max_timesteps=300)\n",
    "num_episodes = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor and critic network constructors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "\n",
    "def actor_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values'])\n",
    "    )\n",
    "\n",
    "def critic_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q actor-critic using estimated returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent configuration, without any optional extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for n in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of episode returns: occasionally successful, but generally not stable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of actor loss: consistently relatively high, due to being weighted by cumulative return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of critic loss: converging, occasionally increasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q actor-critic using normalized estimated returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent configuration, this time with `normalize_returns` set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of episode returns: consistently improving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of actor loss: consistently around zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of critic loss: converging to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is interesting to compare this to the performance of our policy-gradient algorithm with normalized returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PG(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    network_fn=actor_fn, learning_rate=1e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")\n",
    "\n",
    "# Training loop\n",
    "returns = list()\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})\n",
    "\n",
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD actor-critic using estimated advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, compute_advantage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of episode returns: improving, occasionally unstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of actor loss: converging to zero very quickly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot of critic loss: converging to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FrozenLake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_threshold: 0.78, optimum: 0.8196, max_timesteps: 100\n",
    "env = OpenAIGym(level='FrozenLake', max_timesteps=100)\n",
    "num_episodes = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_shaping_fn(reward, terminal, state):\n",
    "    if terminal == 1 and reward == 0.0:\n",
    "        return -1.0, terminal\n",
    "    elif terminal == 2 and reward == 0.0:\n",
    "        return -0.5, terminal\n",
    "    else:\n",
    "        return reward, terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 16\n",
    "\n",
    "def actor_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values']),\n",
    "    )\n",
    "\n",
    "def critic_fn():\n",
    "    return torch.nn.Sequential(\n",
    "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
    "        torch.nn.Tanh(),\n",
    "        torch.nn.Linear(in_features=hidden_size, out_features=1),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q actor-critic using estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=3e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q actor-critic using normalized estimated returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, normalize_returns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TD actor-critic using estimated advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = AC(\n",
    "    state_space=env.state_space, action_space=env.action_space,\n",
    "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
    "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
    "    discount=0.95, compute_advantage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = list()\n",
    "actor_loss = list()\n",
    "critic_loss = list()\n",
    "\n",
    "# Training loop\n",
    "pbar = tqdm(range(num_episodes))\n",
    "pbar.set_postfix({'return': 'n/a'})\n",
    "for _ in pbar:\n",
    "    returns.append(0.0)\n",
    "\n",
    "    # Episode loop\n",
    "    state = env.reset()\n",
    "    terminal = 0\n",
    "    while terminal == 0:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, terminal = env.step(action)\n",
    "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
    "        state = next_state\n",
    "        returns[-1] += reward\n",
    "        if updated:\n",
    "            actor_loss.append(agent.last_actor_loss_value)\n",
    "            critic_loss.append(agent.last_critic_loss_value)\n",
    "\n",
    "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(actor_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_utils.draw_loss_chart(critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
