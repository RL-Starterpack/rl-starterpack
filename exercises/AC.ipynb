{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "AC.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LojKEAuSWYEF"
      },
      "source": [
        "# RL Tutorial - **AC Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80Kx5mOxWYEF"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "OMSPBHiMWYEF"
      },
      "source": [
        "#@title Run this cell to clone the RL tutorial repository and install it\n",
        "try:\n",
        "  import rl_starterpack\n",
        "  print('RL-Starterpack repo succesfully installed!')\n",
        "except ImportError:\n",
        "  print('Cloning RL-Starterpack package...')\n",
        "\n",
        "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
        "  print('Installing RL-StarterPack package...')\n",
        "  !pip install -e rl-starterpack &> /dev/null\n",
        "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "  print('Please restart the runtime to use the newly installed package!')\n",
        "  print('Runtime > Restart Runtime')\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "-MD_29DnWYEF"
      },
      "source": [
        "#@title Run this cell to install additional dependencies (will take ~30s)\n",
        "!pip install torchviz > /dev/null\n",
        "!pip install gym pyvirtualdisplay > /dev/null\n",
        "!apt-get remove ffmpeg > /dev/null # Removing due to restrictive license\n",
        "!apt-get install -y xvfb python-opengl > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "Uf9vBEm-WYEF"
      },
      "source": [
        "#@title Run this cell to import the required libraries\n",
        "try:\n",
        "    from rl_starterpack import AC, OpenAIGym, PG, experiment, vis_utils\n",
        "except ImportError:\n",
        "    print('Please run the first cell! If you already ran it, make sure '\n",
        "          'to restart the runtime after the package is installed.')\n",
        "    raise\n",
        "\n",
        "import gym\n",
        "from itertools import chain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torchviz\n",
        "from tqdm.auto import tqdm\n",
        "%matplotlib inline\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Setup display to show video renderings\n",
        "if 'display' not in globals():\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocFWB19gWYEF"
      },
      "source": [
        "You've seen most of these steps before, so we'll quickly go through them here, focusing on various actor-critic configurations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I53seRl-WYEF"
      },
      "source": [
        "## CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpqMV2yyWYEF"
      },
      "source": [
        "Setup CartPole environment:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W3yBe6X3WYEF"
      },
      "source": [
        "env = OpenAIGym(level='CartPole', max_timesteps=300)\n",
        "num_episodes = 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3x50NS6PWYEF"
      },
      "source": [
        "Actor and critic network constructors:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqo9ATHIWYEF"
      },
      "source": [
        "hidden_size = 16\n",
        "\n",
        "def actor_fn():\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values'])\n",
        "    )\n",
        "\n",
        "def critic_fn():\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Linear(in_features=env.state_space['shape'][0], out_features=hidden_size),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(in_features=hidden_size, out_features=1)\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYMQpdUXWYEF"
      },
      "source": [
        "### Q actor-critic using estimated returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmQ9Y1DmWYEF"
      },
      "source": [
        "Agent configuration, without any optional extensions:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3_hQE3fnWYEF"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BlGkGLO_WYEF"
      },
      "source": [
        "Training loop:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "AXi-KgiiWYEF"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for n in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0YigzLoWYEF"
      },
      "source": [
        "Plot of episode returns: occasionally successful, but generally not stable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "cud2hClTWYEF"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_qimM8oWYEF"
      },
      "source": [
        "Plot of actor loss: consistently relatively high, due to being weighted by cumulative return"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "y1bxX_GjWYEF"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tv-YA_Q3WYEF"
      },
      "source": [
        "Plot of critic loss: converging, occasionally increasing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "iWTtY4sjWYEF"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJrOtIPXWYEF"
      },
      "source": [
        "Visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7zq4pK3WYEF"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFTKsQnTWYEF"
      },
      "source": [
        "### Q actor-critic using normalized estimated returns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldKyNxNPWYEF"
      },
      "source": [
        "Agent configuration, this time with `normalize_returns` set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lp6EynxjWYEF"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95, normalize_returns=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2T1IGqcWYEG"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5grLk24WYEG"
      },
      "source": [
        "Plot of episode returns: consistently improving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "Kb9GRM7hWYEG"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbYUZvWEWYEG"
      },
      "source": [
        "Plot of actor loss: consistently around zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Pef3KWl4WYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTNgUlbdWYEG"
      },
      "source": [
        "Plot of critic loss: converging to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKsJG5KQWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S8GznXuuWYEG"
      },
      "source": [
        "It is interesting to compare this to the performance of our policy-gradient algorithm with normalized returns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OO0_0tuHWYEG"
      },
      "source": [
        "agent = PG(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    network_fn=actor_fn, learning_rate=1e-3,\n",
        "    discount=0.95, normalize_returns=True\n",
        ")\n",
        "\n",
        "# Training loop\n",
        "returns = list()\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})\n",
        "\n",
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2LQiB6YWYEG"
      },
      "source": [
        "Visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMYFo8UFWYEG"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDC8W37tWYEG"
      },
      "source": [
        "### TD actor-critic using estimated advantage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km_fs3vQWYEG"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95, compute_advantage=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OG2macFAWYEG"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HPngpMeWYEG"
      },
      "source": [
        "Plot of episode returns: improving, occasionally unstable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "GVfehQ8MWYEG"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hw4XY8NzWYEG"
      },
      "source": [
        "Plot of actor loss: converging to zero very quickly"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-xrBp_L8WYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqPvQIsoWYEG"
      },
      "source": [
        "Plot of critic loss: converging to zero"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHB8mbC4WYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sz65I40WYEG"
      },
      "source": [
        "Visualization:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vwpiu2D7WYEG"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ho4A_4GvWYEG"
      },
      "source": [
        "## FrozenLake"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2cGeUkQWYEG"
      },
      "source": [
        "# reward_threshold: 0.78, optimum: 0.8196, max_timesteps: 100\n",
        "env = OpenAIGym(level='FrozenLake', max_timesteps=100)\n",
        "num_episodes = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yzVxhe3gWYEG"
      },
      "source": [
        "def reward_shaping_fn(reward, terminal, state):\n",
        "    if terminal == 1 and reward == 0.0:\n",
        "        return -1.0, terminal\n",
        "    elif terminal == 2 and reward == 0.0:\n",
        "        return -0.5, terminal\n",
        "    else:\n",
        "        return reward, terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43ilPbmoWYEG"
      },
      "source": [
        "hidden_size = 16\n",
        "\n",
        "def actor_fn():\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(in_features=hidden_size, out_features=env.action_space['num_values']),\n",
        "    )\n",
        "\n",
        "def critic_fn():\n",
        "    return torch.nn.Sequential(\n",
        "        torch.nn.Embedding(num_embeddings=env.state_space['num_values'], embedding_dim=hidden_size),\n",
        "        torch.nn.Tanh(),\n",
        "        torch.nn.Linear(in_features=hidden_size, out_features=1),\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qo-ltBHlWYEG"
      },
      "source": [
        "### Q actor-critic using estimated returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kaq7S2clWYEG"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=3e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUhJPSBHWYEG"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upmXMK_lWYEG"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0ysvMszWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "UPUKFhO-WYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In_pafnbWYEG"
      },
      "source": [
        "### Q actor-critic using normalized estimated returns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PDxPHzhWYEG"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95, normalize_returns=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jECOmNwiWYEG"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-fy8rGiWYEG"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXVQkh0xWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "wzWdNfOyWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_neiY4JzWYEG"
      },
      "source": [
        "### TD actor-critic using estimated advantage"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GKIP7uyWYEG"
      },
      "source": [
        "agent = AC(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    actor_fn=actor_fn, actor_learning_rate=1e-3,\n",
        "    critic_fn=critic_fn, critic_learning_rate=3e-3,\n",
        "    discount=0.95, compute_advantage=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyaGMqG7WYEG"
      },
      "source": [
        "returns = list()\n",
        "actor_loss = list()\n",
        "critic_loss = list()\n",
        "\n",
        "# Training loop\n",
        "pbar = tqdm(range(num_episodes))\n",
        "pbar.set_postfix({'return': 'n/a'})\n",
        "for _ in pbar:\n",
        "    returns.append(0.0)\n",
        "\n",
        "    # Episode loop\n",
        "    state = env.reset()\n",
        "    terminal = 0\n",
        "    while terminal == 0:\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, terminal = env.step(action)\n",
        "        updated = agent.observe(state, action, reward, terminal, next_state)\n",
        "        state = next_state\n",
        "        returns[-1] += reward\n",
        "        if updated:\n",
        "            actor_loss.append(agent.last_actor_loss_value)\n",
        "            critic_loss.append(agent.last_critic_loss_value)\n",
        "\n",
        "    pbar.set_postfix({'return': '{:.2f}'.format(returns[-1])})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7zXJT0SZWYEG"
      },
      "source": [
        "vis_utils.draw_returns_chart(returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j95ntfyUWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(actor_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NljAfLbsWYEG"
      },
      "source": [
        "vis_utils.draw_loss_chart(critic_loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKfcPQYqWYEG"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}