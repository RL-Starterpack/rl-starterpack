{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/DQN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYdi-Zojwu4Y"
      },
      "source": [
        "# RL Tutorial - **DQN Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJNYGD7N0Bw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hBeWqe2-ws3o"
      },
      "source": [
        "#@title Run this cell to clone the RL tutorial repository and install it\n",
        "try:\n",
        "  import rl_starterpack\n",
        "  print('RL-Starterpack repo succesfully installed!')\n",
        "except ImportError:\n",
        "  print('Cloning RL-Starterpack package...')\n",
        "\n",
        "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
        "  print('Installing RL-StarterPack package...')\n",
        "  !pip install -e rl-starterpack[full] &> /dev/null\n",
        "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "  print('Please restart the runtime to use the newly installed package!')\n",
        "  print('Runtime > Restart Runtime')\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PjALQSLQLrAN"
      },
      "source": [
        "#@title Run this cell to install additional dependencies (will take ~30s)\n",
        "!apt-get remove ffmpeg > /dev/null # Removing due to restrictive license\n",
        "!apt-get install -y xvfb x11-utils > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oDyXosiDM93i"
      },
      "source": [
        "#@title Run this cell to import the required libraries\n",
        "try:\n",
        "    from rl_starterpack import OpenAIGym, DQN, experiment, vis_utils\n",
        "except ImportError:\n",
        "    print('Please run the first cell! If you already ran it, make sure to restart the runtime after the package is installed.')\n",
        "    raise\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import torchviz\n",
        "from itertools import chain\n",
        "%matplotlib inline\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Setup display to show video renderings\n",
        "if 'display' not in globals():\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIeBogMPNyK_"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajgfz89qNeiP"
      },
      "source": [
        "### FrozenLake: DQN Style!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYB74Z4vSpHT"
      },
      "source": [
        "#### Neural Network\n",
        "We define our FrozenLake environment as in TQL. Let's use the non-stochastic version to make sure that our new DQN approach using a neural network works."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2DBxfMkqNeD8"
      },
      "source": [
        "env = OpenAIGym(level='FrozenLake', max_timesteps=100, is_slippery=False) # Non-stochastic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baqRfWfVOB2Z"
      },
      "source": [
        "Now we define the heart of our DQN: the neural network. Don't worry if you're not familiar with PyTorch, the code does the following:\n",
        "1. Build an input **Embedding Layer**. This functions as a lookup table which maps the integer representation of states (provided by the environment) to a learned vector per state. So the lookup table has shape `num_states * embedding_dimension`. \n",
        "2. Pass the embedding layer output through a **hyperbolic tangent non-linearity**.\n",
        "3. Finally, a linear layer maps these to a final vector with one Q-value per action.\n",
        "\n",
        "Feel free to modify the neural network by adding layers, increasing the hidden size, or changing the non-linearity."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Khcujk2PVqtW"
      },
      "source": [
        "num_states = env.state_space['num_values']\n",
        "num_actions = env.action_space['num_values']\n",
        "hidden_size = 16  # The \"width\" of the neural network\n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Embedding(num_embeddings=num_states, \n",
        "                       embedding_dim=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1I7qmoo_BAI"
      },
      "source": [
        "Using this neural network constructor, we now create the agent. Fill in the hyperparameters below and see if your agent succeeds in the next section!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hsWiNN-pRccE"
      },
      "source": [
        "# TODO: Fill in these hyperparameters\n",
        "learning_rate = None  # Speed at which the agent learns. Between (0,1)\n",
        "discount_rate = None  # How much future rewards are discounted at each step. Between (0,1)\n",
        "exploration = None  # During training the agent will take a random action and \"explore\" with this probability. Between (0,1)\n",
        "\n",
        "agent = DQN(\n",
        "    state_space=env.state_space, action_space=env.action_space, network_fn=network_fn,\n",
        "    learning_rate=learning_rate, discount=discount_rate, exploration=exploration\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "jvYGwseCRXUN"
      },
      "source": [
        "#@title _<sub><sup>SOLUTION: Expand this cell to see working parameters for the learning rate, discount_rate and exploration in the non-stochastic environment </sup></sub>_\n",
        "learning_rate = 1e-3  # Speed at which the agent learns. Between (0,1)\n",
        "discount_rate = 0.95  # How much future rewards are discounted at each step. Between (0,1)\n",
        "exploration = 0.25  # During training the agent will take a random action and \"explore\" with this probability. Between (0,1)\n",
        "\n",
        "\n",
        "agent = DQN(\n",
        "    state_space=env.state_space, action_space=env.action_space, network_fn=network_fn,\n",
        "    learning_rate=learning_rate, discount=discount_rate, exploration=exploration\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJnmPLPgSHuz"
      },
      "source": [
        "We can call our DQN to get Q-values for a given state. Recall that by outputting all actions for a given state, we save ourselves from having to call the network for every state-action pair. Compare this to the TQL tabular approach. Note that the network isn't trained yet, so the output values aren't yet meaningful."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWapfgX8yaon"
      },
      "source": [
        "random_state = np.random.randint(0, num_states)\n",
        "agent.network(torch.tensor(random_state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCW-Z49-RfJl"
      },
      "source": [
        "We can inspect what the neural network looks like:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiFHvgfoctR1"
      },
      "source": [
        "named_params = dict(agent.network.named_parameters())\n",
        "torchviz.make_dot(agent.network(torch.tensor(random_state)), params=named_params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4a93IJsdI-n"
      },
      "source": [
        "####  Train and evaluate on non-stochastic environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDxszeY6dw_-"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=500)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=500)\n",
        "print('Mean eval return:', sum(eval_returns) / len(eval_returns))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DfR-iabMg_2S"
      },
      "source": [
        "vis_utils.draw_returns_chart(train_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YP1NH7VXaMW"
      },
      "source": [
        "We can also inspect how our agent solved the environment!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrWaWwNKd4zy"
      },
      "source": [
        "experiment.evaluate_render(agent, env, ipythondisplay, sleep=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lokCHnbqJYzY"
      },
      "source": [
        "#### Train and evaluate on stochastic \"slippery\" environment\n",
        "\n",
        "Now that we know that our implementation works on a non-stochastic environment, let us try it on the true \"slippery\" environment and see if it can solve it. Similarly to TQL, we may need to provide some additional reward shaping to help it solve the environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_rDnjPTJ_tV"
      },
      "source": [
        "env = OpenAIGym(level='FrozenLake', max_timesteps=100, is_slippery=True) # stochastic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NmO6--ItJ2eI"
      },
      "source": [
        "def reward_shaping_fn(reward, terminal, next_state):\n",
        "    \"\"\"\n",
        "    Shapes the reward before passing it on to the agent.\n",
        "    Args:\n",
        "        reward (float): Reward returned by the environment for the action which was just performed.\n",
        "        terminal (int): Boolean int representing whether the current episode has ended (if episode has ended =1, otherwise =0).\n",
        "        next_state (object): Next state. In the case of FrozenLake this is a np.ndarray of a scalar. i.e. np.array(0)\n",
        "    Returns:\n",
        "        reward (float): The modified rewarad.\n",
        "        terminal (int): The `terminal` input needs to be passed through.\n",
        "    \"\"\"\n",
        "    # TODO: Fill in if your agent is having a hard time solving the environment!\n",
        "    return reward, terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbRsWzpR1bzP"
      },
      "source": [
        "# TODO: Fill in these hyperparameters\n",
        "learning_rate = None  # Speed at which the agent learns. Between (0,1)\n",
        "discount_rate = None  # How much future rewards are discounted at each step. Between (0,1)\n",
        "exploration = None  # During training the agent will take a random action and \"explore\" with this probability. Between (0,1)\n",
        "\n",
        "agent = DQN(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    network_fn=network_fn,\n",
        "    learning_rate=learning_rate, \n",
        "    discount=discount_rate, \n",
        "    exploration=exploration\n",
        ")\n",
        "\n",
        "experiment.train(agent, env, num_episodes=1000, \n",
        "                 reward_shaping_fn=reward_shaping_fn)\n",
        "returns = experiment.evaluate(agent, env, num_episodes=1000)\n",
        "print('Mean return:', returns.mean())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9PIqudAeSzb"
      },
      "source": [
        "We can also inspect how our agent solved the environment. Note that since the environment is slippery, the agent may not solve it everytime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aAWn1Kw4pzE"
      },
      "source": [
        "experiment.evaluate_render(agent, env, ipythondisplay, sleep=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "ag9x4qYS9Bkd"
      },
      "source": [
        "#@title _<sub><sup>SOLUTION: Expand this cell to see a working DQN implementation </sup></sub>_\n",
        "\n",
        "# Environment\n",
        "env = OpenAIGym(level='FrozenLake', max_timesteps=100, is_slippery=True) # stochastic\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "discount_rate = 0.95\n",
        "exploration = 0.1\n",
        "\n",
        "# Define our agent\n",
        "num_states = env.state_space['num_values']\n",
        "num_actions = env.action_space['num_values']\n",
        "hidden_size = 16  # The \"width\" of the neural network\n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Embedding(num_embeddings=num_states, \n",
        "                       embedding_dim=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
        "))\n",
        "\n",
        "def agent_fn():\n",
        "    return DQN(\n",
        "        state_space=env.state_space, action_space=env.action_space, network_fn=network_fn,\n",
        "        learning_rate=learning_rate, discount=discount_rate, exploration=exploration\n",
        "    )\n",
        "\n",
        "# Provide some helpful reward shaping\n",
        "def reward_shaping_fn(reward, terminal, next_state):\n",
        "    del next_state # unused\n",
        "    if terminal == 1 and reward == 0.0:\n",
        "        # Penalize the agent for failing to reach the goal\n",
        "        return -1.0, terminal\n",
        "    else:\n",
        "        return reward, terminal\n",
        "\n",
        "# Train the agent. We should be able to achieve a reward of >0.7\n",
        "experiment.train(agent, env, num_episodes=1000, \n",
        "                 reward_shaping_fn=reward_shaping_fn)\n",
        "returns = experiment.evaluate(agent, env, num_episodes=1000)\n",
        "print(f'Run Returns: {returns.mean():.3f} Â± {returns.std():.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqKz78u3ZmK5"
      },
      "source": [
        "### CartPole"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OW1ZdUqRYxZ_"
      },
      "source": [
        "Next we'll be looking at an environment with a continuous state space: [CartPole](https://gym.openai.com/envs/CartPole-v1/)\n",
        "\n",
        "> A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center\n",
        "\n",
        "Recall that one of the advantages DQN provides over TQL is the ability to handle such continuous state space environments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxQfqDQU0YMJ"
      },
      "source": [
        "env = OpenAIGym('CartPole', max_timesteps=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-GxDBHsBcV9k"
      },
      "source": [
        "print('State Information (angles are in rad)\\n')\n",
        "pd.DataFrame.from_dict({'Observation': ['Cart Position', 'Cart Velocity', 'Pole Angle', 'Pole Angular Velocity'],\n",
        "                        'min_value': env.state_space['min_value'],\n",
        "                        'max_value': env.state_space['max_value'], \n",
        "                        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyYU2MFZzs2X"
      },
      "source": [
        "#### Neural Network\n",
        "We now define our new DQN network to handle this new environment with a continuous state space.\n",
        "Note that we no longer need to embed the input state space, since it's now a vector of 4 continuous observations (see table above)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KPi5-jQZqkb"
      },
      "source": [
        "state_obs_dim = env.state_space['shape'][0]  # Size of vector representing the state observations\n",
        "num_actions = env.action_space['num_values']\n",
        "hidden_size = 16 \n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=state_obs_dim, out_features=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
        "))\n",
        "agent = DQN(\n",
        "    env.state_space, env.action_space, network_fn=network_fn, \n",
        "    discount=0.9, exploration=0.05, learning_rate=1e-3\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmetPFUO0H7s"
      },
      "source": [
        "#### Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hCklPE4c0Ev8"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=1500)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_yN6MOWhBHl"
      },
      "source": [
        "vis_utils.draw_returns_chart(train_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdPiqx2zQan"
      },
      "source": [
        "Let's visualise our performance! If your agent managed to balance the pole for >300 timesteps, try increasing max_timesteps below to see if it balance indefinitely. Also note that since the cartpole has a random starting state, it might not solve it everytime, so try running the visualisation a few times to see different episodes.\n",
        "\n",
        "_Note: to show videos in Colab, we need to render them as gifs._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLrEjBwWhRD"
      },
      "source": [
        "# To show longer balancing run this before creating the gif: env = OpenAIGym('CartPole', max_timesteps=500)\n",
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta7xcHVD2H7L"
      },
      "source": [
        "### DQN Extensions\n",
        "There are a few improvements to vanilla DQN which improve the stability of the learning process. If you've found that the learning of the agents above aren't stable, then these may help. We've made a couple of these available in our implementation for you to play with. Feel free to combine them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xNIC8ENv4iRf"
      },
      "source": [
        "#### Experience Replay\n",
        "\n",
        "Recall from the slides that the idea here is to save \"experiences\" in a memory (also known as a replay buffer) and then randomly sample batches of experiences from this memory to update the network. This has the following advantages:\n",
        "\n",
        "\n",
        "*   Batch updates are less noisy, more stable and much faster!\n",
        "*   Items within a batch are less correlated since they are sampled across multiple episodes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FlykxaH6HVQ"
      },
      "source": [
        "env = OpenAIGym('CartPole', max_timesteps=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEZG70RW2LhS"
      },
      "source": [
        "agent = DQN(\n",
        "    env.state_space, env.action_space, network_fn=network_fn, \n",
        "    discount=0.9, exploration=0.05, learning_rate=1e-3,\n",
        "    # Replay memory params\n",
        "    memory=100, # Size of the replay memory. Must be >= batch size.\n",
        "    batch_size=16,\n",
        "    update_frequency=4, # The frequency at which the network is updated (i.e. how often a batch is sampled to update the network)\n",
        "    update_start=None  # Number of timesteps to collect before first update. Must be >= batch size. If None then = batch size.\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEH9YFDa56vd"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=1500)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=500)\n",
        "vis_utils.draw_returns_chart(train_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8w5htrS6caJ"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XLVP_s6x4Xmi"
      },
      "source": [
        "#### Target Network\n",
        "Recall that the idea here is to use a separate Q-network to estimate the TD-target. The network is then infrequently synced with the main network. The advantage of this is that it reduces correlation between the Q-value and the TD-target. You can think of this as temporarily \"fixing\" our goal (the TD-target), so that we don't have a moving target to chase."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukAJKdF26jhK"
      },
      "source": [
        "env = OpenAIGym('CartPole', max_timesteps=300)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spo7Oxm_4WKE"
      },
      "source": [
        "# The q-network and target network are identical and are both created using the network_fn input\n",
        "agent = DQN(\n",
        "    env.state_space, env.action_space, network_fn=network_fn, \n",
        "    discount=0.9, exploration=0.05, learning_rate=1e-3,\n",
        "    # Target network params\n",
        "    target_network_update_frequency=15, # The frequency at which the target network is updated\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RsAsbPCc8m2J"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=1500)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=500)\n",
        "vis_utils.draw_returns_chart(train_returns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kl_x8k956iIi"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "1RLyjAwI8znS"
      },
      "source": [
        "#@title _<sub><sup>SOLUTION: Everything together! </sup></sub>_\n",
        "env = OpenAIGym('CartPole', max_timesteps=300)\n",
        "\n",
        "state_obs_dim = env.state_space['shape'][0]  # Size of vector representing the state observations\n",
        "num_actions = env.action_space['num_values']\n",
        "hidden_size = 16 \n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=state_obs_dim, out_features=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
        "))\n",
        "\n",
        "agent = DQN(state_space=env.state_space, \n",
        "            action_space=env.action_space, network_fn=network_fn,\n",
        "            discount=0.9, exploration=0.1, learning_rate=1e-3,\n",
        "            target_network_update_frequency=10, \n",
        "            memory=500, \n",
        "            batch_size=16,\n",
        "            update_start=100,\n",
        "            update_frequency=4)\n",
        "\n",
        "train_returns = experiment.train(agent, env, num_episodes=2000)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=1000)\n",
        "vis_utils.draw_returns_chart(train_returns)\n",
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQSu43Q63gRs"
      },
      "source": [
        "## Leaderboard\n",
        "\n",
        "Once you have completed the exercises above consider submitting your scores to our DQN leaderboard using [this form](https://forms.gle/oM3yJJmz7nQfwavJ9).\n",
        "\n",
        "Note: to compute the \"mean evaluation return\" you can do `eval_returns.mean()`."
      ]
    }
  ]
}