{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PG.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/PG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wanacOZupYKT"
      },
      "source": [
        "# RL Tutorial - **Policy Gradient Exercise**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFJNYGD7N0Bw"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "hBeWqe2-ws3o"
      },
      "source": [
        "#@title Run this cell to clone the RL tutorial repository and install it\n",
        "try:\n",
        "  import rl_starterpack\n",
        "  print('RL-Starterpack repo succesfully installed!')\n",
        "except ImportError:\n",
        "  print('Cloning RL-Starterpack package...')\n",
        "\n",
        "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
        "  print('Installing RL-StarterPack package...')\n",
        "  !pip install -e rl-starterpack[full] &> /dev/null\n",
        "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "  print('Please restart the runtime to use the newly installed package!')\n",
        "  print('Runtime > Restart Runtime')\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "PjALQSLQLrAN"
      },
      "source": [
        "#@title Run this cell to install additional dependencies (will take ~30s)\n",
        "!apt-get remove ffmpeg > /dev/null # Removing due to restrictive license\n",
        "!apt-get install -y xvfb x11-utils > /dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "oDyXosiDM93i"
      },
      "source": [
        "#@title Run this cell to import the required libraries\n",
        "try:\n",
        "  from rl_starterpack import OpenAIGym, PG, experiment, vis_utils\n",
        "except ImportError:\n",
        "  print('Please run the first cell! If you already ran it, make sure to restart the runtime after the package is installed.\\n')\n",
        "  raise\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import gym\n",
        "import torchviz\n",
        "from itertools import chain\n",
        "%matplotlib inline\n",
        "from pyvirtualdisplay import Display\n",
        "from IPython import display as ipythondisplay\n",
        "\n",
        "# Setup display to show video renderings\n",
        "if 'display' not in globals():\n",
        "    display = Display(visible=0, size=(1400, 900))\n",
        "    display.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1_7NiYmpVu7"
      },
      "source": [
        "## Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oktexbrp5NW"
      },
      "source": [
        "### My Policy Gradient Agent\n",
        "As a first step, we will be implementing two key parts of the Policy Gradient agent:\n",
        "\n",
        "1. The `reward_to_go` function to be used in the REINFORCE algorithm.\n",
        "1. The `surrogate_loss` function to be used for gradient descent.\n",
        "\n",
        "Recall from our REINFORCE algorithm:\n",
        "\n",
        "\n",
        ">For each timestep in our trajectory $(t= 1, ..., T)$:\n",
        "  1. Compute the `reward_to_go`: $\\sum_{k=t}^{T}\\gamma^{k-t}R(k)$\n",
        "  1. Update the policy parameters using: $\\theta\\gets\\theta+\\alpha R(t)\\nabla_\\theta \\log(\\pi_\\theta(a_t | s_t))$\n",
        "\n",
        "And that we minimize a `surrogate_loss` to give us the desired gradient above:\n",
        "\n",
        "> $L(s,a) = -R(\\tau)\\log(\\pi_\\theta(a | s))$\n",
        "\n",
        "In practice we use the mean across a whole rollout:\n",
        "\n",
        "> $Loss = \\frac{-\\sum_{t=1}^{T}R(t)\\log(\\pi_\\theta(a_t | s_t)}{T}$\n",
        "\n",
        "Where here $R(t)$ is the `reward_to_go` at timestep $t$.\n",
        "\n",
        "If you need a refresher, you can refer back to the slides on the [tutorial home page](http://rl-starterpack.github.io/). If you're curious about the full implementation, it can be found in the [repo for this tutorial](https://github.com/RL-Starterpack/rl-starterpack/blob/main/rl_starterpack/agents/pg.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LosP7Ar-qygt"
      },
      "source": [
        "# TODO: Fill in these two methods\n",
        "class MyPolicyGradient(PG):\n",
        "  @staticmethod\n",
        "  def reward_to_go(reward, terminal, discount):\n",
        "      \"\"\"\n",
        "      Reward to go implementation.\n",
        "      Args:\n",
        "          reward (np.ndarray[float]): 1-D array of rewards for the current, complete episode rollout. For example:\n",
        "              [0., 0., 0., 0., 1.] would represent a rollout with 5 timesteps where the agent received a reward of 1\n",
        "              at the last timestep and a reward of 0 at every other timetstep.\n",
        "          terminal (np.ndarray[int]): 1-D array representing whether or not the timestep is \"terminal\" (i.e. the \n",
        "              episode has finished). Same dimension as `reward`. For example:\n",
        "              [0., 0., 0., 0., 1.] would represent a rollout with 5 timesteps where the last timestep is terminal.\n",
        "          discount (float): Discount factor in the range [0,1].\n",
        "      Returns:\n",
        "          np.ndarray: 1-D array representing the reward to go at every timestep. Should have the same dimensions as \n",
        "              `reward` and `terminal`.\n",
        "      \"\"\"\n",
        "      raise NotImplementedError\n",
        "\n",
        "  @staticmethod\n",
        "  def surrogate_loss(rollout_value, log_prob):\n",
        "      \"\"\"\n",
        "      Surrogate loss implementation.\n",
        "      Args:\n",
        "          rollout_value (torch.Tensor[float]): 1-D array of reward_to_go, as computed by reward_to_go.\n",
        "          log_prob (torch.Tensor[float]): 1-D array of log probabilities for the agent actions given the current state.\n",
        "              Same dimension as rollout value.\n",
        "      Returns:\n",
        "          torch.Tensor[float]: Scalar of the current loss. Note you may need to use the `my_tensor.mean()` function.\n",
        "      \"\"\"\n",
        "      raise NotImplementedError"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZwMbjkVl91H"
      },
      "source": [
        "Run the cells below to test that your implementation works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln4gfxPanOdD"
      },
      "source": [
        "def test_reward_to_go(pg):\n",
        "    reward = np.array([0, 0.2, 0.5, 0.8])\n",
        "    terminal = np.array([0, 0, 0, 1])\n",
        "    discount = 0.9\n",
        "    r2g = pg.reward_to_go(reward, terminal, discount)\n",
        "    expected_r2g = np.array([1.1682, 1.298 , 1.22  , 0.8])\n",
        "\n",
        "    assert isinstance(r2g, np.ndarray)\n",
        "    np.testing.assert_allclose(r2g, expected_r2g) \n",
        "\n",
        "def test_surrogate_loss(pg):\n",
        "    rollout_value = torch.tensor([0.5, 1.5, 2.0, 3.5])\n",
        "    log_prob = torch.tensor([-0.5, -0.6, -0.7, -0.8])\n",
        "    loss = pg.surrogate_loss(rollout_value, log_prob)\n",
        "    expected_loss = torch.tensor(1.3375)\n",
        "\n",
        "    assert isinstance(loss, torch.Tensor)\n",
        "    assert torch.allclose(loss, expected_loss) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E51Ecys6z608"
      },
      "source": [
        "tests_pass = False\n",
        "try:\n",
        "    test_reward_to_go(MyPolicyGradient)\n",
        "    test_surrogate_loss(MyPolicyGradient)\n",
        "    tests_pass = True\n",
        "except NotImplementedError:\n",
        "    print('Your implementation is incomplete!\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "KZtHGGiNtcYo"
      },
      "source": [
        "#@title _<sub><sup>SOLUTION: Expand this cell to see a solution for the MyPolicyGradient implementation </sup></sub>_\n",
        "class MyPolicyGradientSolution(PG):\n",
        "    @staticmethod\n",
        "    def reward_to_go(reward, terminal, discount):\n",
        "        num_timesteps = reward.shape[0]\n",
        "        rollout_value = reward\n",
        "        for n in range(num_timesteps - 2, -1, -1):\n",
        "            if terminal[n] == 0:\n",
        "                rollout_value[n] += discount * rollout_value[n + 1]\n",
        "        return rollout_value\n",
        "\n",
        "    @staticmethod\n",
        "    def surrogate_loss(rollout_value, log_prob):\n",
        "        # Surrogate loss:  -(R[t:] * log pi(s_t, a_t))\n",
        "        return -(rollout_value * log_prob).mean()\n",
        "\n",
        "test_reward_to_go(MyPolicyGradientSolution)\n",
        "test_surrogate_loss(MyPolicyGradientSolution)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoQzaVGK3jQT"
      },
      "source": [
        "# If the tests pass, then we use your implementation here on out, otherwise use repo implementation\n",
        "if tests_pass:\n",
        "    PolicyGradient = MyPolicyGradient\n",
        "else:\n",
        "    PolicyGradient = PG"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7S7FXmVpiWe"
      },
      "source": [
        "### CartPole... Again! \n",
        "Now let's test that our agent works on cartpole."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aMZF2sPWkrV7"
      },
      "source": [
        "env = OpenAIGym('CartPole', max_timesteps=300)  # We set this to 300 to prevent training from taking too long, but feel free to increase this "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSodQgRU3prG"
      },
      "source": [
        "We model our policy with a neural network which takes in the current state as the input and outputs a distribution over actions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nREkiGa53wtE"
      },
      "source": [
        "state_obs_dim = env.state_space['shape'][0]  # Size of vector representing the state observations\n",
        "num_actions = env.action_space['num_values']\n",
        "hidden_size = 16 \n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=state_obs_dim, out_features=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=num_actions)\n",
        "))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Mehm9fE4BDs"
      },
      "source": [
        "We can inspect this input-output relationship by calling our network. The output corresponds to the logits (unnormalized predictions) of the actions: apply force left or right."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykbek6Rw34xx"
      },
      "source": [
        "example_policy_network = network_fn()\n",
        "zero_state = np.zeros(shape=env.state_space['shape'], dtype=np.float32)\n",
        "example_policy_network(torch.tensor(zero_state))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvPy7Ei04Rt0"
      },
      "source": [
        "Now let's build our agent with this policy network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOcZ_MzyxYcN"
      },
      "source": [
        "agent = PolicyGradient(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    network_fn=network_fn, learning_rate=1e-3,\n",
        "    discount=0.99, \n",
        "    normalize_returns=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLIkmOW-y8Vr"
      },
      "source": [
        "Note: You may notice that above we have `normalize_returns=True`, which subtracts the mean and divides by the standard deviation of the returns within the current episode. In practice, normalizing returns helps ensure stability by controlling the variance of the policy gradient estimator. More information can be found on [Andrej Karpathy's blog](http://karpathy.github.io/2016/05/31/rl/) under \"More general advantage functions\". Feel free to try the agent without the normalization and see how it performs!\n",
        "\n",
        "Reward normalization can also be seen as a very simple form of advantage actor critic (A2C), since it similarly normalizes the variance and centers the returns around 0. We will see A2C in the next tutorial section, where we discuss actor-critic (AC) methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PELLqAoT0PNp"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=1000)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=500)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ty56R9c0oV-"
      },
      "source": [
        "vis_utils.draw_returns_chart(train_returns, smoothing_window=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jFdPiqx2zQan"
      },
      "source": [
        "Let's visualise our performance! If your agent managed to balance the pole for >300 timesteps, try increasing max_timesteps below to see if it balance indefinitely. Also note that since the cartpole has a random starting state, it might not solve it everytime, so try running the visualisation a few times to see different episodes.\n",
        "\n",
        "_Note: to show videos in Colab, we need to render them as gifs._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hLrEjBwWhRD"
      },
      "source": [
        "# To show longer balancing run this before creating the gif: env = OpenAIGym('CartPole', max_timesteps=500)\n",
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19iP3bHT3BSO"
      },
      "source": [
        "As you may have noticed, Policy Gradient tends to converge faster (and more consistently) than DQN."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KveZiCrw2isC"
      },
      "source": [
        "### Pendulum\n",
        "\n",
        "Recall that one of the advantages of Policy Gradient is that we can work with continuous action spaces. So let's try our implementation on an environment with a continuous action space: [Pendulum](https://gym.openai.com/envs/Pendulum-v0/)\n",
        "\n",
        "> The inverted pendulum swingup problem is a classic problem in the control literature. In this version of the problem, the pendulum starts in a random position, and the goal is to swing it up so it stays upright."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ONMIKdOt3dUy"
      },
      "source": [
        "env = OpenAIGym('Pendulum', max_timesteps=100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-1_4KnSN6KRj"
      },
      "source": [
        "print('Action Information\\n')\n",
        "pd.DataFrame.from_dict(env.action_space)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqHKU6pR1Zk5"
      },
      "source": [
        "print('State Information (theta is pole angle from vertical) \\n')\n",
        "pd.DataFrame.from_dict({'Observation': ['cos(theta)', 'sin(theta)', 'angular velocity'],\n",
        "                        'min_value': env.state_space['min_value'],\n",
        "                        'max_value': env.state_space['max_value'], \n",
        "                        })"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trE6r6fy2uWH"
      },
      "source": [
        "Now we define our PG network and run it on the new environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Of7wzv5N2tww"
      },
      "source": [
        "state_obs_dim = env.state_space['shape'][0]  # Size of vector representing the state observations\n",
        "action_dim = env.action_space['shape'][0]\n",
        "hidden_size = 16 \n",
        "\n",
        "network_fn = (lambda: torch.nn.Sequential(\n",
        "    torch.nn.Linear(in_features=state_obs_dim, out_features=hidden_size),\n",
        "    torch.nn.Tanh(),\n",
        "    torch.nn.Linear(in_features=hidden_size, out_features=action_dim)\n",
        "))\n",
        "\n",
        "agent = PolicyGradient(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    network_fn=network_fn, learning_rate=1e-3,\n",
        "    discount=0.9, \n",
        "    normalize_returns=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mLTfqF_n46nQ"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=2000)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=100)\n",
        "vis_utils.draw_returns_chart(train_returns, smoothing_window=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11ZiZQpo5BFx"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehyG7c474-t0"
      },
      "source": [
        "Most likely the agent failed to \"swing up\" the pendulum. Try running the visualisation a few times to see how it performs with different random pole initialisations.\n",
        "\n",
        "\n",
        "The \"natural\" environment reward is a negative cost, so the best we can do is 0 reward. The cost is a function of the angle (higher if the pole is further from being upright), angular momentum (higher for higher angular momentum) and the applied torque (higher for more torque).\n",
        "\n",
        "We can do somewhat better than our attempt above, by using our own reward shaping function. This function rewards the agent for having high angular momentum at the bottom half of the pendulum swing, and making it to the top half of the swing with less momentum."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uwt0BqHK4JFR"
      },
      "source": [
        "def reward_shaping_fn(reward, terminal, state):\n",
        "    cos_theta, sin_theta, ang_momentum = state\n",
        "    if 0 <= cos_theta <= 1: # upper half\n",
        "        return (25 - np.abs(ang_momentum)*0.5)*5, terminal\n",
        "    return cos_theta + np.abs(ang_momentum)*0.5, terminal  # cos_theta will be negative here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxCgkgbj3dU7"
      },
      "source": [
        "env = OpenAIGym('Pendulum', max_timesteps=100)\n",
        "agent = PolicyGradient(\n",
        "    state_space=env.state_space, action_space=env.action_space,\n",
        "    network_fn=network_fn, learning_rate=1e-3,\n",
        "    discount=0.9, \n",
        "    normalize_returns=True\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZmfCpVP8g9t"
      },
      "source": [
        "train_returns = experiment.train(agent, env, num_episodes=2000, reward_shaping_fn=reward_shaping_fn)\n",
        "eval_returns = experiment.evaluate(agent, env, num_episodes=100)\n",
        "vis_utils.draw_returns_chart(train_returns, smoothing_window=50)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jEFVj5q19gWh"
      },
      "source": [
        "vis_utils.show_episode_as_gif(ipythondisplay, agent, env)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCvvPEHE5d7R"
      },
      "source": [
        "Once again try running the visualisation a few times to see how it performs with different random pole initialisations.\n",
        "\n",
        "As you've most likely observed, it ends up doing somewhat better but far from solves the environment. It turns out that this is quite a challenging problem, because the agent can't just \"react\" to the observed pole position. It needs to learn how the torque it applies to the pole will impact its position and angular momentum and plan around that. To solve this, we may need to use some more advanced techniques (a solution using Deep Deterministic Policy Gradient can be found [here](https://github.com/lirnli/OpenAI-gym-solutions/blob/master/Continuous_Deep_Deterministic_Policy_Gradient_Net/DDPG%20Class%20ver2%20(Pendulum-v0).ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gz4FQ6b3L8x4"
      },
      "source": [
        "If youv'e finished early feel free to play around with the hyperparameters, neural network and reward function above to see if you can find a better solution to the Pendulum environment!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRpFJwhe6Nsy"
      },
      "source": [
        "## Leaderboard\n",
        "\n",
        "Once you have completed the exercises above consider submitting your scores to our PG leaderboard using [this form](https://forms.gle/jKaNZKS9vkBaWY9y7).\n",
        "\n",
        "Note: to compute the \"mean evaluation return\" you can do `eval_returns.mean()`."
      ]
    }
  ]
}