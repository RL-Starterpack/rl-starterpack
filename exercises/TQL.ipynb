{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/TQL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYdi-Zojwu4Y"
   },
   "source": [
    "# RL Tutorial - **TQL Exercise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFJNYGD7N0Bw"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "hBeWqe2-ws3o"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to clone the RL tutorial repository and install it\n",
    "try:\n",
    "  import rl_starterpack\n",
    "  print('RL-Starterpack repo succesfully installed!')\n",
    "except ImportError:\n",
    "  print('Cloning RL-Starterpack package...')\n",
    "\n",
    "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
    "  print('Installing RL-StarterPack package...')\n",
    "  !pip install -e rl-starterpack[full] &> /dev/null\n",
    "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "  print('Please restart the runtime to use the newly installed package!')\n",
    "  print('Runtime > Restart Runtime')\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "PjALQSLQLrAN"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to install additional dependencies (will take ~30s)\n",
    "!apt-get remove ffmpeg > /dev/null # Removing due to restrictive license\n",
    "!apt-get install -y xvfb x11-utils > /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "oDyXosiDM93i"
   },
   "outputs": [],
   "source": [
    "#@title Run this cell to import the required libraries\n",
    "try:\n",
    "    from rl_starterpack import OpenAIGym, TQL, experiment, vis_utils\n",
    "except ImportError:\n",
    "    print('Please run the first cell! If you already ran it, make sure to restart the runtime after the package is installed.')\n",
    "    raise\n",
    "from itertools import chain\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import pandas as pd\n",
    "import altair as alt\n",
    "import torch\n",
    "import gym\n",
    "import torchviz\n",
    "%matplotlib inline\n",
    "from pyvirtualdisplay import Display\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "# Setup display to show video renderings\n",
    "if 'display' not in globals():\n",
    "    display = Display(visible=0, size=(1400, 900))\n",
    "    display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fIeBogMPNyK_"
   },
   "source": [
    "## Exercise\n",
    "\n",
    "### FrozenLake: Tabular Q-learning method\n",
    "First we are going to see how RL works from the outside-in. Later we will get to grips with the details of the TQL method.\n",
    "\n",
    "The RL starterpack repository contains agent implementations as well as helper code to run experiments and train agents.\n",
    "We will use the repository's implementation of tabular Q-learning to demonstrate how this code fits together and how we visualise the results.\n",
    "\n",
    "#### Environment and TQL agent\n",
    "We set up our environment and a constructor function to create a Tabular Q-learning agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VifUX1ZzoUrh"
   },
   "outputs": [],
   "source": [
    "env = OpenAIGym(level='FrozenLake', max_timesteps=100)\n",
    "\n",
    "def agent_fn():\n",
    "    return TQL(\n",
    "        state_space=env.state_space, action_space=env.action_space,\n",
    "        learning_rate=0.3, discount=0.9, exploration=0.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iSU5pezzoUrh"
   },
   "source": [
    "The environment limits episodes to 100 time-steps.\n",
    "We need this limit, as otherwise agents' policies can sometimes get stuck in infinite loops.\n",
    "The agent's parameters are:\n",
    "\n",
    "  - `learning_rate`: a \"step size\" for the temporal difference update\n",
    "  - `discount`: a factor that determines how rewards are temporally discounted\n",
    "  - `exploration`: a rate that controls the agent's balance between exploration and exploitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iw62R3CToUrh"
   },
   "outputs": [],
   "source": [
    "num_runs = 5                  # number of training +evaluation loops we run\n",
    "num_episodes_train = 1000     # number of training episodes per run\n",
    "num_episodes_eval = 37        # number of evaluation episodes per run\n",
    "pbar = tqdm(range(num_runs))  # This wraps the run iterator with a progress bar\n",
    "pbar.set_postfix({'mean return': 'n/a'})\n",
    "run_returns = list()\n",
    "for run in pbar:\n",
    "    # Create and train an agent\n",
    "    agent = agent_fn()\n",
    "    _ = experiment.train(agent, env, num_episodes_train, use_pbar=True)\n",
    "\n",
    "    # Evaluation loop\n",
    "    eval_returns = experiment.evaluate(agent, env, num_episodes_eval, use_pbar=True)\n",
    "    pbar.set_postfix({'mean return': '{:.2f}'.format(eval_returns.mean())})\n",
    "\n",
    "    # Close agent\n",
    "    agent.close()\n",
    "\n",
    "    # Record evaluation return\n",
    "    run_returns.append(pd.DataFrame(data=dict(evaluation=np.arange(num_episodes_eval),\n",
    "                                              run=run,\n",
    "                                              eval_return=eval_returns)))\n",
    "    \n",
    "# Combine data frames\n",
    "run_returns = pd.concat(run_returns).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4emiK8XLoUrh"
   },
   "source": [
    "What returns do you expect to see from each episode? Run the next block to see if you are right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fq4OD5CUoUrh"
   },
   "outputs": [],
   "source": [
    "run_returns.sample(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9qZ-k6DoUri"
   },
   "source": [
    "Now we can examine the variation in returns across training runs and evaluation episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvZG1Dc9oUri"
   },
   "outputs": [],
   "source": [
    "alt.Chart(run_returns).mark_rect().encode(\n",
    "    x='evaluation:O',\n",
    "    y='run:O',\n",
    "    color='eval_return:O'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n-LnO9iZoUri"
   },
   "source": [
    "We see that there is variation in the success rate between the training runs.\n",
    "That is, some training runs appear to have resulted in more or less successful agents.\n",
    "Also, we note that due to the stochastic nature of the environment, each agent has variation in the returns across evaluation episodes.\n",
    "\n",
    "We can calculate the means of the evaluation returns and their standard errors for each training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uudPQQigoUri"
   },
   "outputs": [],
   "source": [
    "run_returns.groupby('run')['eval_return'].agg([np.mean, st.sem])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kbsv4wDoUri"
   },
   "source": [
    "We can also examine how the agent from the last training run solves the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrWaWwNKd4zy"
   },
   "outputs": [],
   "source": [
    "experiment.evaluate_render(agent, env, ipythondisplay, sleep=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71-kFLO2oUri"
   },
   "source": [
    "Your results may vary but more than likely it is not an impressive solution. The agent takes many wrong steps.\n",
    "\n",
    "### Tune the hyperparameters\n",
    "\n",
    "Tuning the hyperparameters is one thing we can try to improve our agents performance.\n",
    "Fill in some values for the hyperparameters below to investigate how this affects the mean return.\n",
    "\n",
    "Remember that even for fixed values of the hyperparameters the results will vary every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cbRsWzpR1bzP"
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in these hyperparameters\n",
    "learning_rate = None  # Speed at which the agent learns. Between (0,1)\n",
    "discount_rate = None  # How much future rewards are discounted at each step. Between (0,1)\n",
    "exploration = None    # During training the agent will take a random action and \"explore\" with this probability.\n",
    "                      # Between (0,1)\n",
    "\n",
    "# Create the agent with the given parameters\n",
    "agent = TQL(state_space=env.state_space, action_space=env.action_space,\n",
    "            learning_rate=learning_rate, discount=discount_rate, exploration=exploration)\n",
    "\n",
    "# Train the agent\n",
    "train_returns = experiment.train(agent, env, num_episodes=1000)\n",
    "# experiment.train(agent, env, num_episodes=1000, reward_shaping_fn=reward_shaping_fn)\n",
    "\n",
    "# Evaluate the agent\n",
    "returns = experiment.evaluate(agent, env, num_episodes=100)\n",
    "print(f'Mean return: {returns.mean():.3f} +/- {st.sem(returns):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ihhwFkW5oUri"
   },
   "source": [
    "Do you have a good understanding of what each parameter does?\n",
    "\n",
    "We can visualise the returns achieved during training.\n",
    "The blue line are the raw returns and the orange line is a smoothed version of the raw returns, so any trend is apparent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5lE2uCOoUri"
   },
   "outputs": [],
   "source": [
    "vis_utils.draw_returns_chart(train_returns, smoothing_window=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NVdxr-noUri"
   },
   "source": [
    "High values for the exploration parameter will decrease the mean training return. Why?\n",
    "\n",
    "Let's examine how our new agent solves the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "At-f8vPqoUri"
   },
   "outputs": [],
   "source": [
    "experiment.evaluate_render(agent, env, ipythondisplay, sleep=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQvny4JvoUri"
   },
   "source": [
    "Hopefully this new agent has learnt a better policy. Your mileage may vary, but it is unlikely it has reached a perfect solution yet.\n",
    "\n",
    "### Reward shaping\n",
    "\n",
    "Another way to help the agent learn a better policy is a method called reward shaping.\n",
    "This is useful when the reward signal that the environment provides is not optimal for learning.\n",
    "In this Frozen Lake environment, landing on a hole terminates the episode and provides a reward of 0.\n",
    "A reward of 0 is the same as for other non-goal states, and so it does not signal to the agent that this outcome should be avoided.\n",
    "\n",
    "A *reward shaping function* takes the reward provided by the environment and amends it to improve learning.\n",
    "In the Frozen Lake environment, a reward of -1 for landing on a hole might be a better signal for the agent.\n",
    "Fill in the function below to see if training improves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmO6--ItJ2eI"
   },
   "outputs": [],
   "source": [
    "def reward_shaping_fn(reward, terminal, next_state):\n",
    "    \"\"\"\n",
    "    Shapes the reward before passing it on to the agent.\n",
    "    \n",
    "    Args:\n",
    "        reward (float): Reward returned by the environment for the action which was just performed.\n",
    "        terminal (int): Boolean int representing whether the current episode has ended (if episode has ended =1, otherwise =0).\n",
    "        next_state (object): Next state. In the case of FrozenLake this is a np.ndarray of a scalar. i.e. np.array(0)\n",
    "        \n",
    "    Returns:\n",
    "        reward (float): The modified reward.\n",
    "        terminal (int): The `terminal` input needs to be passed through.\n",
    "    \"\"\"\n",
    "    # TODO: Fill in if your agent is having a hard time solving the environment!\n",
    "    \n",
    "    return reward, terminal\n",
    "\n",
    "# Create a new agent with the existing parameters\n",
    "agent = TQL(state_space=env.state_space, action_space=env.action_space,\n",
    "            learning_rate=learning_rate, discount=discount_rate, exploration=exploration)\n",
    "\n",
    "# Train the agent using reward shaping\n",
    "train_returns = experiment.train(agent, env, num_episodes=1000, reward_shaping_fn=reward_shaping_fn)\n",
    "\n",
    "# Evaluate the agent\n",
    "returns = experiment.evaluate(agent, env, num_episodes=100)\n",
    "print(f'Mean return: {returns.mean():.3f} +/- {st.sem(returns):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bq3UuPJ6oUri"
   },
   "source": [
    "Hopefully your mean return is now higher! Returns above 0.7 are possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JbBOlA-ZoUri"
   },
   "outputs": [],
   "source": [
    "#@title _<sub><sup>SOLUTION: Expand this cell to see a working TQL implementation </sup></sub>_\n",
    "\n",
    "# Provide some helpful reward shaping\n",
    "def reward_shaping_fn(reward, terminal, next_state):\n",
    "    del next_state # unused\n",
    "    if terminal == 1 and reward == 0.0:\n",
    "        # Penalize the agent for failing to reach the goal\n",
    "        return -1.0, terminal\n",
    "    else:\n",
    "        return reward, terminal\n",
    "\n",
    "# Create a new agent with the existing parameters\n",
    "agent = TQL(state_space=env.state_space, action_space=env.action_space,\n",
    "            learning_rate=learning_rate, discount=discount_rate, exploration=exploration)\n",
    "\n",
    "# Train the agent using reward shaping\n",
    "train_returns = experiment.train(agent, env, num_episodes=1000, reward_shaping_fn=reward_shaping_fn)\n",
    "\n",
    "# Evaluate the agent\n",
    "returns = experiment.evaluate(agent, env, num_episodes=100)\n",
    "print(f'Mean return: {returns.mean():.3f} +/- {st.sem(returns):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-uNgo4NooUri"
   },
   "source": [
    "If you wish to investigate TQL further, please have a look at the implementation of our [TQL agent](https://github.com/RL-Starterpack/rl-starterpack/blob/main/rl_starterpack/agents/tql.py).\n",
    "In particular look at the `TQL` class that implements `exploration_policy` and `q_learning_policy`.\n",
    "Feel free to implement your own agent that redefines these methods in any way you see fit."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "TQL.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
