{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('scrabble': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "interpreter": {
   "hash": "69eca7ac2b7598d12d280c69b1fb62ad78f1808ab788f27f111baed930593b79"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# RL Tutorial - **Bandits Exercise**\n",
    "\n",
    "\n",
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to clone the RL tutorial repository and install it\n",
    "try:\n",
    "  import rl_starterpack\n",
    "  print('RL-Starterpack repo succesfully installed!')\n",
    "except ImportError:\n",
    "  print('Cloning RL-Starterpack package...')\n",
    "\n",
    "  # !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
    "  print('Installing RL-StarterPack package...')\n",
    "  !pip install -e rl-starterpack[full] &> /dev/null\n",
    "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "  print('Please restart the runtime to use the newly installed package!')\n",
    "  print('Runtime > Restart Runtime')\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "from IPython.display import Markdown as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "As a first step, let's do some experimentation setup. The crucial thing we need is a **test bed**. The test bed is the object where we define the arms, the reward distributions for each arm and the operation of drawing an arm and returning the ensuing reward.\n",
    "\n",
    "We will provide some parameters to play with. Specifically, we can change the number of arms and the parameters of the distribution from which the mean reward for each arm is sampled.\n",
    "\n",
    "We will also define the sampling functions that specify how the mean rewards for each arm is assigned and how the observed rewards are randomly sampled."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "num_arms = 10               # number of arms in the test bed\n",
    "reward_mean = 1             # mean of normal distribution from which mean reward of each arm will be drawn\n",
    "reward_std = 1              # std of normal distribution from which mean reward of each arm will be drawn\n",
    "                            # we will keep the std fixed throughout\n",
    "\n",
    "def arm_mean_sampler(size: int=1):\n",
    "    return np.random.normal(loc=reward_mean, scale=reward_std, size=size)\n",
    "\n",
    "def arm_reward_sampler(mu: float, size: int=1):\n",
    "    reward = np.random.normal(loc=mu, scale=reward_std, size=size)\n",
    "    if size == 1:\n",
    "        return reward[0]\n",
    "    return reward\n",
    "\n",
    "class TestBed:\n",
    "\n",
    "    def __init__(self,\n",
    "                num_arms: int=10,\n",
    "                arm_mean_sampler: callable=arm_mean_sampler,\n",
    "                arm_reward_sampler: callable=arm_reward_sampler) -> None:\n",
    "        self.seed = np.random.choice(1000000)\n",
    "        self.arm_mean_sampler = arm_mean_sampler\n",
    "        self.arm_reward_sampler = arm_reward_sampler\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        self.mean_rewards = arm_mean_sampler(size=num_arms)\n",
    "\n",
    "    def draw_arm(self, idx: int) -> float:\n",
    "        mu = self.mean_rewards[idx]\n",
    "        return self.arm_reward_sampler(mu)\n",
    "\n",
    "    def visualise(self) -> None:\n",
    "        dummies = np.random.normal(0, 1, 1000)\n",
    "        dfs = []\n",
    "        for arm, mu in enumerate(self.mean_rewards):\n",
    "            dfs.append(pd.DataFrame({'Arm': arm, 'Reward Distribution': dummies + mu}))\n",
    "\n",
    "        dfs = pd.concat(dfs)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.violinplot(data=dfs, x='Arm', y='Reward Distribution', width=0.3, ax=ax)\n",
    "        plt.show()\n"
   ]
  },
  {
   "source": [
    "### The Testbed\n",
    "\n",
    "The testbed can be visualised with `testbed.visualise()` to check the distribution of rewards for each arm in the testbed. Keep a note of the arm with the highest actual mean reward. Ideally, we would like each of the policies we implement to find this optimal arm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbed = TestBed(num_arms=num_arms, arm_mean_sampler=arm_mean_sampler, arm_reward_sampler=arm_reward_sampler)\n",
    "testbed.visualise()\n",
    "\n",
    "md(f'In the testbed above, the **optimal arm** is arm **{testbed.mean_rewards.argmax()}**')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, testbed: TestBed, policy: callable, **parameters) -> None:\n",
    "        self.testbed = testbed\n",
    "        self.policy = policy\n",
    "        self.init_rewards = parameters.get('init_rewards', 0)\n",
    "        self._init_fields()\n",
    "\n",
    "    def _init_fields(self) -> None:\n",
    "        self.observed_rewards = []\n",
    "        self.expected_rewards = self.init_rewards * np.ones(self.testbed.num_arms)\n",
    "        self.observed_draws = []\n",
    "        self.num_draws = np.zeros(self.testbed.num_arms, dtype=np.int32)\n",
    "        self.total_draws = 0\n",
    "\n",
    "    def step(self) -> Tuple[int, float]:\n",
    "        chosen_arm = self.policy(self.expected_rewards, self.num_draws)\n",
    "        reward = self.testbed.draw_arm(chosen_arm)\n",
    "\n",
    "        self.total_draws += 1\n",
    "        self.observed_draws.append(chosen_arm)\n",
    "        self.num_draws[chosen_arm] += 1\n",
    "        self.observed_rewards.append(reward)\n",
    "        self.expected_rewards[chosen_arm] += (\n",
    "            (1. / (self.num_draws[chosen_arm] + 1)) * (reward - self.expected_rewards[chosen_arm]))\n",
    "\n",
    "        return chosen_arm, reward\n",
    "\n",
    "    def run_episode(self, num_steps: int) -> None:\n",
    "        for i in range(num_steps):\n",
    "            _ = self.step()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._init_fields()\n",
    "\n",
    "    def visualise(self, kind='line') -> None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 10), nrows=3, ncols=1)\n",
    "\n",
    "        reward_df = pd.DataFrame({\n",
    "                            'timestep': range(len(self.observed_rewards)),\n",
    "                            'reward': self.observed_rewards})\n",
    "        if kind=='line':\n",
    "            sns.lineplot(data=reward_df, x='timestep', y='reward', ax=ax[0])\n",
    "        else:\n",
    "            sns.scatterplot(data=reward_df, x='timestep', y='reward', hue='reward', ax=ax[0])\n",
    "\n",
    "        sns.violinplot(x=self.observed_draws, y=self.observed_rewards, width=0.5, ax=ax[1])\n",
    "        ax[1].set_ylabel('reward')\n",
    "        ax[1].set_xlabel('arm')\n",
    "\n",
    "        arms, counts = np.unique(self.observed_draws, return_counts=True)\n",
    "        sns.scatterplot(x=arms, y=counts, ax=ax[2])\n",
    "        ax[2].set_ylim(0, len(self.observed_rewards))\n",
    "        ax[2].set_ylabel('draws')\n",
    "        ax[2].set_xlabel('arm')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def average_experiments(policy: callable,\n",
    "                        init_rewards: Union[int, float]=0,\n",
    "                        timesteps: int=1000, repeats: int=200,\n",
    "                        **parameters) -> None:\n",
    "    for k, v in parameters.items():\n",
    "        if not isinstance(v, (list, tuple)):\n",
    "            parameters[k] = [v]\n",
    "\n",
    "    parameters = ParameterGrid(parameters)\n",
    "    results = []\n",
    "    mean_rewards = []\n",
    "    optimal_arm_draws = []\n",
    "\n",
    "\n",
    "    for param_set in parameters:\n",
    "        param_set_string = ', '.join(f'{k}={v}' for k, v in param_set.items())\n",
    "        observed_rewards = []\n",
    "        for n in tqdm(range(repeats)):\n",
    "            testbed = TestBed()\n",
    "            optimal_arm = testbed.mean_rewards.argmax()\n",
    "            policy = partial(policy, **param_set)\n",
    "            experiment = Experiment(testbed=testbed, policy=policy, **param_set)\n",
    "            experiment.run_episode(num_steps=timesteps)\n",
    "            observed_rewards.append(experiment.observed_rewards)\n",
    "            optimal_arm_draws.append((n, experiment.num_draws[optimal_arm], param_set_string))\n",
    "            mean_rewards.append((n, np.array(experiment.observed_rewards).mean(), param_set_string))\n",
    "        \n",
    "        observed_rewards = np.array(observed_rewards).T.flatten()\n",
    "        rewards_df = pd.DataFrame({\n",
    "                                'timestep': np.repeat(range(timesteps), repeats),\n",
    "                                'reward': observed_rewards,\n",
    "                                'params': param_set_string\n",
    "                    })\n",
    "        results.append(rewards_df)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 10), nrows=3, ncols=1)\n",
    "\n",
    "    results = pd.concat(results)\n",
    "    sns.lineplot(data=results,\n",
    "                x='timestep',\n",
    "                y='reward',\n",
    "                hue='params',\n",
    "                ax=ax[0])\n",
    "    mean_rewards = pd.DataFrame(mean_rewards, columns=['iteration', 'mean reward', 'params'])\n",
    "    sns.violinplot(data=mean_rewards,\n",
    "                x='params',\n",
    "                y='mean reward',\n",
    "                ax=ax[1])\n",
    "    optimal_arm_draws = pd.DataFrame(optimal_arm_draws, columns=['iteration', 'optimal draws', 'params'])\n",
    "    sns.violinplot(data=optimal_arm_draws,\n",
    "                x='params',\n",
    "                y='optimal draws',\n",
    "                ax=ax[2])\n",
    "    plt.show()"
   ]
  },
  {
   "source": [
    "## The EXPERIMENT\n",
    "\n",
    "The `Experiment` object will help us run and accumulate the results of a sequence of arm pulls of a testbed. We will provide a **testbed** and a **policy** to the **experiment**. We will pull arms from the testbed using the policy for a fixed number of timesteps. After that we can visualise the results. Specifically, we will look at the profile of rewards received over time according to the policy, the distribution of rewards observed from each arm and the number of times each arm was pulled in the experiment. To quantify the quality of the policies we implement we will also look the optimal arm found by each policy and the average reward accumulated over the experiment's time horizon.\n",
    "\n",
    "## POLICIES\n",
    "\n",
    "### Explore Then Commit\n",
    "In this policy we systematically explore each arm for a fixed number of rounds. After that, we exploit the best known arm for the remaining timesteps.\n",
    "\n",
    "The hyperparameter for this policy will be the **number of rounds** for which we explore the arms. We can set this value to be anything between `1` (explore the arms for one round only) and `horizon / number_of_arms` (keep exploring the arms throughout).\n",
    "\n",
    "Set the hyperparameter and implement the policy in the following cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement policy here\n",
    "num_rounds = 50\n",
    "\n",
    "def explore_then_commit(expected_rewards: List[float], num_draws: List[int], num_rounds: int=num_rounds) -> int:\n",
    "    num_arms = len(expected_rewards)\n",
    "    total_draws = sum(num_draws)\n",
    "    if total_draws < num_rounds * num_arms:\n",
    "        return total_draws % num_arms\n",
    "\n",
    "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, explore_then_commit)\n",
    "experiment.run_episode(num_steps=1000)\n",
    "experiment.visualise()\n",
    "\n",
    "md(f'''\n",
    "Optimal arm found with **ETC** is **{experiment.expected_rewards.argmax()}**\n",
    "\n",
    "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
    "\n",
    "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiments(explore_then_commit, num_rounds=[25, 50])"
   ]
  },
  {
   "source": [
    "### $\\epsilon$-greedy\n",
    "In this policy, we either uniformly choose a random arm with a probability $ \\epsilon \\in (0,1)$ or choose the arm with best known expected reward."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.1\n",
    "\n",
    "# IMPLEMENT policy here\n",
    "def epsilon_greedy(expected_rewards: List[float], num_draws: List[int], epsilon: float=epsilon) -> int:\n",
    "    if np.random.uniform() <= epsilon:\n",
    "        choices = range(len(expected_rewards))\n",
    "    else:\n",
    "        choices = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "        \n",
    "    arm = np.random.choice(choices)\n",
    "    return arm\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, epsilon_greedy)\n",
    "experiment.run_episode(num_steps=1000)\n",
    "experiment.visualise()\n",
    "\n",
    "md(f'''\n",
    "Optimal arm found with **$\\epsilon$-greedy** is **{experiment.expected_rewards.argmax()}**\n",
    "\n",
    "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
    "\n",
    "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiments(epsilon_greedy, epsilon=[0.01, 0.1, 1])"
   ]
  },
  {
   "source": [
    "### Optimistic Initial Values (**OIV**)\n",
    "In this policy, we set the initial expected reward values to a **wildly optimistic high number**. The reasoning is, as we update the high initial expected rewards with the observed rewards, they will tend to lower values. In this setting, if we choose the arm with the highest expected reward, we will end up exploring the arms more until the arm with the best reward is found."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimistic_initial_reward = 5\n",
    "\n",
    "# IMPLEMENT policy here\n",
    "def optimistic_initial_values(expected_rewards: List[float], num_draws: int) -> int:\n",
    "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, optimistic_initial_values, init_rewards=optimistic_initial_reward)\n",
    "experiment.run_episode(num_steps=1000)\n",
    "experiment.visualise()\n",
    "\n",
    "md(f'''\n",
    "Optimal arm found with **OIV** is **{experiment.expected_rewards.argmax()}**\n",
    "\n",
    "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
    "\n",
    "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiments(optimistic_initial_values, init_rewards=[1, 5, 10])"
   ]
  },
  {
   "source": [
    "### Upper Confidence Bound (**UCB**)\n",
    "In this policy, we use the number of times each arm has been drawn to compute a confidence interval for the expected reward for each arm. At each step we select the arm with the highest upper bound for the confidence interval. The **weight given to uncertainty term**, $c \\in (0, 1]$ is a hyperparameter in this case.\n",
    "\n",
    "This method is the most popular policy used for bandits. The uncertainty bounds aid is exploration in the initial stages, but stops it as soon as soon as the optimal arm is found.\n",
    "\n",
    "One thing to keep in mind is UCB can only be applied after each arm has been tried at least once. This is to ensure that uncertainty for each arm can be computed. For this we add another hyperparameter for the **number of tries**. Until each arm has been pulled this minimum number of times, we randomly select one of the arms that has not yet been pulled enough times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT policy here\n",
    "weight = 0.5\n",
    "num_tries_per_arm = 10\n",
    "\n",
    "def upper_confidence_bound(expected_rewards: List[float],\n",
    "                        num_draws: List[int],\n",
    "                        weight: float=weight,\n",
    "                        num_tries_per_arm: int=num_tries_per_arm) -> int:\n",
    "    if any(num_draws < num_tries_per_arm):\n",
    "        unpulled_arms = np.where(num_draws < num_tries_per_arm)[0]\n",
    "        return np.random.choice(unpulled_arms)\n",
    "\n",
    "    total_draws = sum(num_draws)\n",
    "    num_arms = len(expected_rewards)\n",
    "    uncertainty = np.sqrt(np.log(total_draws + num_arms) / (num_draws + num_arms))\n",
    "    ucb = expected_rewards + weight * uncertainty\n",
    "\n",
    "    max_reward_arms = np.where(ucb == max(ucb))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, upper_confidence_bound)\n",
    "experiment.run_episode(num_steps=1000)\n",
    "experiment.visualise()\n",
    "\n",
    "md(f'''\n",
    "Optimal arm found with **UCB** is **{experiment.expected_rewards.argmax()}**\n",
    "\n",
    "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
    "\n",
    "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_experiments(upper_confidence_bound, weight=0.5, num_tries_per_arm=[1, 2, 10])"
   ]
  },
  {
   "source": [
    "### Thompson Sampling\n",
    "In this policy, we are going to put a prior on each arm and learn the distribution of the rewards for each pulled arm directly.\n",
    "\n",
    "To keep things simple, we will look at a Beta/Bernoulli example. For that, we will make a few modifications to our setup.\n",
    "\n",
    "We will start with redifining our mean sampling functions and using them to create a Bernoulli testbed."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arm_mean_sampler(size: int=1) -> float:\n",
    "    return np.random.uniform(size=size)\n",
    "\n",
    "def arm_reward_sampler(p: float, size: int=1) -> float:\n",
    "    reward = np.random.binomial(n=1, p=p, size=size)\n",
    "    if size == 1:\n",
    "        return reward[0]\n",
    "    return reward\n",
    "\n",
    "testbed = TestBed(num_arms=num_arms, arm_mean_sampler=arm_mean_sampler, arm_reward_sampler=arm_reward_sampler)\n",
    "sns.scatterplot(x=range(num_arms), y=testbed.mean_rewards, ax=plt.subplots(figsize=(15, 5))[1])\n",
    "plt.ylim(0, 1)\n",
    "plt.xlabel('Arm')\n",
    "plt.ylabel('Mean reward')\n",
    "plt.show()\n",
    "\n",
    "md(f\"Arm with **maximum mean reward** is arm **{testbed.mean_rewards.argmax()}**\")\n"
   ]
  },
  {
   "source": [
    "Now let's redefine how our experiments are conducted for the Beta/Bernoulli use case."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set initial Beta prior parameters\n",
    "alpha = 0\n",
    "beta = 0\n",
    "\n",
    "class ThompsonExperiment(Experiment):\n",
    "    \n",
    "    def _init_fields(self) -> None:\n",
    "        self.observed_rewards = []\n",
    "        # alphas - first column, betas - second column\n",
    "        self.beta_params = np.tile([alpha, beta], self.testbed.num_arms).reshape(self.testbed.num_arms, 2)\n",
    "        self.observed_draws = []\n",
    "        self.num_draws = np.zeros(self.testbed.num_arms, dtype=np.int32)\n",
    "        self.total_draws = 0\n",
    "\n",
    "    def step(self) -> Tuple[int, float]:\n",
    "        chosen_arm = self.policy(self.beta_params)\n",
    "        reward = self.testbed.draw_arm(chosen_arm)\n",
    "\n",
    "        self.total_draws += 1\n",
    "        self.observed_draws.append(chosen_arm)\n",
    "        self.num_draws[chosen_arm] += 1\n",
    "        self.observed_rewards.append(reward)\n",
    "        if reward == 1:\n",
    "            self.beta_params[chosen_arm, 0] += 1\n",
    "        else:\n",
    "            self.beta_params[chosen_arm, 1] += 1\n",
    "\n",
    "        return chosen_arm, reward\n"
   ]
  },
  {
   "source": [
    "Now let's implement the sampling policy. For Thompson sampling, we will use the prior parameters to sample expected rewards for each arm and select the arm with the highest sampled reward. Depending on the observed reward, we update the prior parameters for the respective arm."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_bernoulli_priors(beta_params: List[List[int]]) -> int:\n",
    "    sampled_rewards = []\n",
    "    for i in range(len(beta_params)):\n",
    "        alpha, beta = beta_params[i]\n",
    "        reward = scipy.stats.beta.rvs(alpha + 1, beta + 1)\n",
    "        sampled_rewards.append(reward)\n",
    "\n",
    "    max_reward_arms = np.where(np.array(sampled_rewards) == max(sampled_rewards))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = ThompsonExperiment(testbed, beta_bernoulli_priors)\n",
    "experiment.run_episode(num_steps=1000)\n",
    "experiment.visualise(kind='scatter')\n",
    "\n",
    "expected_means = np.array([scipy.stats.beta.mean(a + 1, b + 1) for a, b in experiment.beta_params])\n",
    "\n",
    "md(f'''\n",
    "Optimal arm found with **Thompson sampling** is **{expected_means.argmax()}**\n",
    "\n",
    "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
    "\n",
    "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
    "''')"
   ]
  }
 ]
}