{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.2 64-bit ('textspy': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  },
  "interpreter": {
   "hash": "1504abe6da4fc60bc53e1688a31a816a02522c02b33df30ebec855d062909500"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# RL Tutorial - **Bandits Exercise**\n",
    "\n",
    "\n",
    "## Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Run this cell to clone the RL tutorial repository and install it\n",
    "try:\n",
    "  import rl_starterpack\n",
    "  print('RL-Starterpack repo succesfully installed!')\n",
    "except ImportError:\n",
    "  print('Cloning RL-Starterpack package...')\n",
    "\n",
    "  # !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
    "  print('Installing RL-StarterPack package...')\n",
    "  !pip install -e rl-starterpack[full] &> /dev/null\n",
    "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
    "  print('Please restart the runtime to use the newly installed package!')\n",
    "  print('Runtime > Restart Runtime')\n",
    "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional, Tuple, Union\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "As a first step, let's do some experimentation setup. The crucial thing we need is a **test bed**. The test bed is the object where we define the arms, the reward distributions for each arm and how to conduct an experiment on the arms.\n",
    "\n",
    "We will provide some parameters to play with. Specifically, we can change the number of arms, the parameters of the distribution from which the "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARAMETERS\n",
    "num_arms = 10               # number of arms in the test bed\n",
    "reward_mean = 1             # mean of normal distribution from which mean reward of each arm will be drawn\n",
    "reward_std = 1              # std of normal distribution from which mean reward of each arm will be drawn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestBed:\n",
    "\n",
    "    def __init__(self, num_arms: int=10, reward_mean: Union[int, float]=0, reward_std: Union[int, float]=1) -> 'TestBed':\n",
    "        self.seed = np.random.choice(1000000)\n",
    "        self.reward_mean = reward_mean\n",
    "        self.reward_std = reward_std\n",
    "        self.num_arms = num_arms\n",
    "\n",
    "        np.random.seed(self.seed)\n",
    "        self.mean_rewards = np.random.normal(reward_mean, reward_std, num_arms)\n",
    "\n",
    "    def draw_arm(self, idx: int) -> float:\n",
    "        mu = self.mean_rewards[idx]\n",
    "        return np.random.normal(mu, self.reward_std)\n",
    "\n",
    "    def visualise(self) -> None:\n",
    "        dummies = np.random.normal(0, 1, 1000)\n",
    "        dfs = []\n",
    "        for arm, mu in enumerate(self.mean_rewards):\n",
    "            dfs.append(pd.DataFrame({'Arm': arm, 'Reward Distribution': dummies + mu}))\n",
    "\n",
    "        dfs = pd.concat(dfs)\n",
    "        fig, ax = plt.subplots(figsize=(10, 5))\n",
    "        sns.violinplot(data=dfs, x='Arm', y='Reward Distribution', width=0.3, ax=ax)\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testbed = TestBed()\n",
    "testbed.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Experiment:\n",
    "    \n",
    "    def __init__(self, testbed: TestBed, policy: callable, init_rewards: Optional[float]=0) -> None:\n",
    "        self.testbed = testbed\n",
    "        self.policy = policy\n",
    "        self.init_rewards = init_rewards\n",
    "        self._init_fields()\n",
    "\n",
    "    def _init_fields(self) -> None:\n",
    "        self.observed_rewards = []\n",
    "        self.expected_rewards = self.init_rewards * np.ones(self.testbed.num_arms)\n",
    "        self.observed_draws = []\n",
    "        self.num_draws = np.zeros(self.testbed.num_arms, dtype=np.int32)\n",
    "        self.total_draws = 0\n",
    "\n",
    "    def step(self) -> Tuple[int, float]:\n",
    "        chosen_arm = self.policy(self.expected_rewards, self.num_draws)\n",
    "        reward = self.testbed.draw_arm(chosen_arm)\n",
    "\n",
    "        self.total_draws += 1\n",
    "        self.observed_draws.append(chosen_arm)\n",
    "        self.num_draws[chosen_arm] += 1\n",
    "        self.observed_rewards.append(reward)\n",
    "        self.expected_rewards[chosen_arm] += (1. / (self.num_draws[chosen_arm] + 1)) * (reward - self.expected_rewards[chosen_arm])\n",
    "\n",
    "        return chosen_arm, reward\n",
    "\n",
    "    def run_episode(self, horizon: int) -> None:\n",
    "        for i in range(horizon):\n",
    "            _ = self.step()\n",
    "\n",
    "    def reset(self) -> None:\n",
    "        self._init_fields()\n",
    "\n",
    "    def visualise(self) -> None:\n",
    "        fig, ax = plt.subplots(figsize=(15, 10), nrows=3, ncols=1)\n",
    "\n",
    "        reward_df = pd.DataFrame({'timestep': range(len(self.observed_rewards)), 'reward': self.observed_rewards})\n",
    "        sns.lineplot(data=reward_df, x='timestep', y='reward', ax=ax[0])\n",
    "\n",
    "        sns.violinplot(x=self.observed_draws, y=self.observed_rewards, width=0.5, ax=ax[1])\n",
    "        ax[1].set_ylabel('reward')\n",
    "        ax[1].set_xlabel('arm')\n",
    "\n",
    "        arms, counts = np.unique(self.observed_draws, return_counts=True)\n",
    "        sns.scatterplot(x=arms, y=counts, ax=ax[2])\n",
    "        ax[2].set_ylim(0, len(self.observed_rewards))\n",
    "        ax[2].set_ylabel('draws')\n",
    "        ax[2].set_xlabel('arm')\n",
    "\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "source": [
    "## POLICIES\n",
    "\n",
    "### Explore Then Commit\n",
    "In this policy we systematically explore each arm for a fixed number of rounds. After that, we exploit the best known arm for the remaining timesteps.\n",
    "\n",
    "The hyperparameter for this policy will be the **number of rounds** for which we explore the arms. We can set this value to be anything between `1` (explore the arms for one round only) and `horizon / number_of_arms` (keep exploring the arms throughout).\n",
    "\n",
    "Set the hyperparameter and implement the policy in the following cell."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement policy here\n",
    "num_rounds = 50\n",
    "\n",
    "def explore_then_commit(expected_rewards: List[float], num_draws: List[int], num_rounds: int=num_rounds) -> int:\n",
    "    num_arms = len(expected_rewards)\n",
    "    total_draws = sum(num_draws)\n",
    "    if total_draws < num_rounds * num_arms:\n",
    "        return total_draws % num_arms\n",
    "\n",
    "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, explore_then_commit)\n",
    "experiment.run_episode(horizon=1000)\n",
    "experiment.visualise()"
   ]
  },
  {
   "source": [
    "### $\\epsilon$-greedy\n",
    "In this policy, we either uniformly choose a random arm with a probability $ \\epsilon \\in (0,1)$ or choose the arm with best known expected reward."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 0.01\n",
    "\n",
    "# IMPLEMENT policy here\n",
    "def epsilon_greedy(expected_rewards: List[float], num_draws: List[int], epsilon: float=epsilon) -> int:\n",
    "    if np.random.uniform() <= epsilon:\n",
    "        choices = range(len(expected_rewards))\n",
    "    else:\n",
    "        choices = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "        \n",
    "    arm = np.random.choice(choices)\n",
    "    return arm\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, epsilon_greedy)\n",
    "experiment.run_episode(horizon=1000)\n",
    "experiment.visualise()"
   ]
  },
  {
   "source": [
    "### Optimistic Initial Values\n",
    "In this policy, we set the initial expected reward values to a **wildly optimistic high number**. The reasoning is, as we update the high initial expected rewards with the observed rewards, they will tend to lower values. In this setting, if we choose the arm with the highest expected reward, we will end up exploring the arms more until the arm with the best reward is found."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "optimistic_initial_reward = 5\n",
    "\n",
    "# IMPLEMENT policy here\n",
    "def optimistic_initial_values(expected_rewards: List[float], num_draws: int) -> int:\n",
    "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, optimistic_initial_values, init_rewards=optimistic_initial_reward)\n",
    "experiment.run_episode(horizon=1000)\n",
    "experiment.visualise()\n"
   ]
  },
  {
   "source": [
    "### Upper Confidence Bound (**UCB**)\n",
    "In this policy, we use the number of times each arm has been drawn to compute a confidence interval for the expected reward for each arm. At each step we select the arm with the highest upper bound for the confidence interval. The **weight given to uncertainty term**, $c \\in (0, 1]$ is a hyperparameter in this case.\n",
    "\n",
    "This method is the most popular policy used for bandits. The uncertainty bounds aid is exploration in the initial stages, but stops it as soon as soon as the optimal arm is found.\n",
    "\n",
    "One thing to keep in mind is UCB can only be applied after each arm has been tried at least once. This is to ensure that uncertainty for each arm can be computed. For this we add another hyperparameter for the **number of tries**. Until each arm has been pulled this minimum number of times, we randomly select one of the arms that has not yet been pulled enough times."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPLEMENT policy here\n",
    "weight = 0.5\n",
    "num_tries_per_arm = 2\n",
    "\n",
    "def upper_confidence_bound(\n",
    "    expected_rewards: List[float],\n",
    "    num_draws: List[int],\n",
    "    weight: float=weight,\n",
    "    num_tries_per_arm: int=num_tries_per_arm) -> int:\n",
    "\n",
    "    if any(num_draws < num_tries_per_arm):\n",
    "        unpulled_arms = np.where(num_draws < num_tries_per_arm)[0]\n",
    "        return np.random.choice(unpulled_arms)\n",
    "\n",
    "    total_draws = sum(num_draws)\n",
    "    num_arms = len(expected_rewards)\n",
    "    uncertainty = np.sqrt(np.log(total_draws + num_arms) / (num_draws + num_arms))\n",
    "    ucb = expected_rewards + weight * uncertainty\n",
    "    max_reward_arms = np.where(ucb == max(ucb))[0]\n",
    "    return np.random.choice(max_reward_arms)\n",
    "\n",
    "# run experiment with policy\n",
    "experiment = Experiment(testbed, upper_confidence_bound)\n",
    "experiment.run_episode(horizon=1000)\n",
    "experiment.visualise()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.num_draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}