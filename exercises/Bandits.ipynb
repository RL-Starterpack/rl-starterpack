{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.5 64-bit ('textspy': conda)"
    },
    "metadata": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    },
    "interpreter": {
      "hash": "952fb1d107255830a21f5b9c7fb7566a5a94a4d2aa709fe51a841383a4dab0d0"
    },
    "colab": {
      "name": "Bandits.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RL-Starterpack/rl-starterpack/blob/main/exercises/Bandits.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OAfR55kZ3fy"
      },
      "source": [
        "# RL Tutorial - **Bandits Exercise**\n",
        "\n",
        "\n",
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "HH3R0ZvkZ3fz"
      },
      "source": [
        "#@title Run this cell to clone the RL tutorial repository and install it\n",
        "try:\n",
        "  import rl_starterpack\n",
        "  print('RL-Starterpack repo succesfully installed!')\n",
        "except ImportError:\n",
        "  print('Cloning RL-Starterpack package...')\n",
        "\n",
        "  !git clone https://github.com/RL-Starterpack/rl-starterpack.git\n",
        "  print('Installing RL-StarterPack package...')\n",
        "  !pip install -e rl-starterpack[full] &> /dev/null\n",
        "  print('\\n\\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')\n",
        "  print('Please restart the runtime to use the newly installed package!')\n",
        "  print('Runtime > Restart Runtime')\n",
        "  print('~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cScqRCFhZ3f0"
      },
      "source": [
        "from typing import List, Optional, Tuple, Union\n",
        "from IPython.display import Markdown as md\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "%matplotlib inline\n",
        "\n",
        "from rl_starterpack import MultiArmedBandit, BanditExperiment, average_runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvStGfQnZ3f0"
      },
      "source": [
        "As a first step, let's do some experimentation setup. The crucial thing we need is a **Multi-Armed Bandit environment**. The environment is the object where we define the arms, the reward distributions for each arm and the operation of drawing an arm and returning the ensuing reward.\n",
        "\n",
        "We will provide some parameters to play with. Specifically, we can change the number of arms and the parameters of the distribution from which the mean reward for each arm is sampled.\n",
        "\n",
        "We will also define the sampling functions that specify how the mean rewards for each arm is assigned and how the observed rewards are randomly sampled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLvQMtN1Z3f0"
      },
      "source": [
        "# PARAMETERS to play around with\n",
        "NUM_ARMS = 10               # number of arms in the test bed\n",
        "REWARD_MEAN = 1             # mean of normal distribution from which mean reward of each arm will be drawn\n",
        "NUM_STEPS = 400             # number of timesteps to perform for each experiment\n",
        "NUM_EXPERIMENTS = 200       # number of independent experiments to average over"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fiL4ZhxZ3f1"
      },
      "source": [
        "# Gaussian samplers\n",
        "def gaussian_mean_sampler(reward_mean: float = REWARD_MEAN, reward_std: float = 1, size: int = 1):\n",
        "    return np.random.normal(loc=reward_mean, scale=reward_std, size=size)\n",
        "\n",
        "\n",
        "def gaussian_reward_sampler(mu: float, reward_std: float = 1, size: int = 1):\n",
        "    reward = np.random.normal(loc=mu, scale=reward_std, size=size)\n",
        "    if size == 1:\n",
        "        return reward[0]\n",
        "    return reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOaJQ72GZ3f1"
      },
      "source": [
        "### The MultiArmedBandit environment\n",
        "\n",
        "The MultiArmedBandit object takes as parameters:\n",
        "\n",
        "1. the number of arms\n",
        "2. the function from which to sample the mean reward of each arm\n",
        "3. the function to sample the observed rewards for each arm given its mean reward\n",
        "\n",
        "The assigned reward distributions for the arms can be visualised with `MultiArmedBandit.visualise()`. Keep a note of the arm with the highest actual mean reward. Ideally, we would like each of the policies we implement to find this optimal arm or another arm with a mean reward very close to optimal. Also keep a watch on how the **average reward** on the same `MultiArmedBandit` changes with the policies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0eR6zIz1Z3f2"
      },
      "source": [
        "multi_armed_bandit = MultiArmedBandit(num_arms=NUM_ARMS, arm_mean_sampler=gaussian_mean_sampler, arm_reward_sampler=gaussian_reward_sampler)\n",
        "multi_armed_bandit.visualise()\n",
        "\n",
        "md(f'In the testbed above, the **optimal arm** is arm **{multi_armed_bandit.mean_rewards.argmax()}**')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Bjfijg8Z3f2"
      },
      "source": [
        "## The EXPERIMENT\n",
        "\n",
        "The `BanditExperiment` object will help us run and accumulate the results of a sequence of arm pulls of a multi armed bandit. We will provide a `MultiArmedBandit` and a `policy` to the `experiment`. We will pull arms from the testbed using the policy for a fixed number of **timesteps**. After that we can visualise the results. Specifically, we will look at the profile of rewards received over time according to the policy, the distribution of rewards observed from each arm and the number of times each arm was pulled in the experiment. To quantify the quality of the policies we implement we will also look at the optimal arm found by each policy and the average reward accumulated over the experiment's timesteps.\n",
        "\n",
        "To make sure our policies actually perform as expected over varying conditions, we will make multiple experiments over different multi armed bandits and average the results to compare the effect of different parameter setting. For this, we have a function `average_runs` which takes the following parameters:\n",
        "\n",
        "1. policy function to pick an arm\n",
        "2. number of timesteps to run each experiment for\n",
        "3. number of separate experiments to run\n",
        "4. parameters for the experiment or policy to compare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jY61dGOZ3f2"
      },
      "source": [
        "## POLICIES\n",
        "\n",
        "We will only focus on some of the policies used with Multi Armed Bandits. In the following, function definitions are provided as an exercise for you to implement. For each policy, we want to return the index of the arm chosen by the policy.\n",
        "\n",
        "**IMPORTANT**: There might be multiple arms with the highest expected reward. If so, then we return any one of the arms with max reward, chosen at random.\n",
        "\n",
        "For each policy, the solution is provided in the following hidden cell."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z36U39lpZ3f3"
      },
      "source": [
        "### Explore Then Commit\n",
        "In this policy we systematically explore each arm for a fixed number of rounds. After that, we exploit the best known arm for the remaining timesteps.\n",
        "\n",
        "The hyperparameter for this policy will be the **number of rounds** for which we explore **each of the arms**. We can set this value to be anything between `1` (explore the arms for one round only) and `timesteps / number_of_arms` (keep exploring the arms throughout).\n",
        "\n",
        "Set the hyperparameter and implement the policy in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJDWH8paZ3f3"
      },
      "source": [
        "# PARAMETERS to play around with\n",
        "num_rounds = 10\n",
        "\n",
        "def explore_then_commit(expected_rewards: List[float], num_draws: List[int], num_rounds: int=num_rounds) -> int:\n",
        "    # IMPLEMENT POLICY HERE\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "wAr7rMnJZ3f3"
      },
      "source": [
        "# @title SOLUTION for ETC policy\n",
        "def explore_then_commit(expected_rewards: List[float], num_draws: List[int], num_rounds: int=num_rounds) -> int:\n",
        "    num_arms = len(expected_rewards)\n",
        "    total_draws = sum(num_draws)\n",
        "    if total_draws < num_rounds * num_arms:\n",
        "        return total_draws % num_arms\n",
        "\n",
        "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
        "    return np.random.choice(max_reward_arms)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdYklvLHZ3f3"
      },
      "source": [
        "# Run the experiment\n",
        "experiment = BanditExperiment(multi_armed_bandit, explore_then_commit)\n",
        "experiment.run(num_steps=NUM_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDPcIkKcZ3f4"
      },
      "source": [
        "# Visualise the results\n",
        "experiment.visualise_results()\n",
        "\n",
        "md(f'''\n",
        "Optimal arm found with **ETC** is **{experiment.num_draws.argmax()}**\n",
        "\n",
        "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
        "\n",
        "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxSDKPN6Z3f4"
      },
      "source": [
        "Now let's compare the policy performance for different values of `num_rounds`, averaged over **200** separate experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3COAqrduZ3f4"
      },
      "source": [
        "average_runs(explore_then_commit, timesteps=NUM_STEPS, repeats=NUM_EXPERIMENTS, num_rounds=[1, 5, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ojquzpAZ3f4"
      },
      "source": [
        "### $\\epsilon$-greedy\n",
        "In this policy, we either uniformly choose a random arm with a probability $ \\epsilon \\in (0,1)$ or choose the arm with best known expected reward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NEe7rTKcZ3f4"
      },
      "source": [
        "# PARAMETER to play around with\n",
        "epsilon = 0.1\n",
        "\n",
        "def epsilon_greedy(expected_rewards: List[float], num_draws: List[int], epsilon: float=epsilon) -> int:\n",
        "    # IMPLEMENT policy here\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpdlYUcBZ3f4"
      },
      "source": [
        "# @title SOLUTION for e-greedy policy\n",
        "def epsilon_greedy(expected_rewards: List[float], num_draws: List[int], epsilon: float=epsilon) -> int:\n",
        "    if np.random.uniform() <= epsilon:\n",
        "        choices = range(len(expected_rewards))\n",
        "    else:\n",
        "        choices = np.where(expected_rewards == max(expected_rewards))[0]\n",
        "        \n",
        "    arm = np.random.choice(choices)\n",
        "    return arm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x-EJUha2Z3f5"
      },
      "source": [
        "# Run the experiment# run experiment with policy\n",
        "experiment = BanditExperiment(multi_armed_bandit, epsilon_greedy)\n",
        "experiment.run(num_steps=NUM_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "thGuQjluZ3f5"
      },
      "source": [
        "# Visualise results\n",
        "experiment.visualise_results()\n",
        "\n",
        "md(f'''\n",
        "Optimal arm found with **$\\epsilon$-greedy** is **{experiment.num_draws.argmax()}**\n",
        "\n",
        "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
        "\n",
        "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6GVusQRZ3f5"
      },
      "source": [
        "Now let's compare the policy performance for different values of `epsilon`, averaged over **200** separate experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8e9CixvZ3f5"
      },
      "source": [
        "average_runs(epsilon_greedy, timesteps=NUM_STEPS, repeats=NUM_EXPERIMENTS, epsilon=[0.01, 0.1, 0.5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lrXPMK9MZ3f5"
      },
      "source": [
        "### Optimistic Initial Values (**OIV**)\n",
        "In this policy, we set the initial expected reward values to a **wildly optimistic high number**. The reasoning is, as we update the high initial expected rewards with the observed rewards, they will tend to lower values. In this setting, if we choose the arm with the highest expected reward, we will end up exploring the arms more until the arm with the best reward is found."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "k_oFYodvZ3f5"
      },
      "source": [
        "# PARAMETER to play around with\n",
        "optimistic_initial_reward = 5\n",
        "\n",
        "def optimistic_initial_values(expected_rewards: List[float], num_draws: int, **kwargs) -> int:\n",
        "    # IMPLEMENT policy here\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G-iad6HZ3f6"
      },
      "source": [
        "# @title SOLUTION for OIV policy\n",
        "def optimistic_initial_values(expected_rewards: List[float], num_draws: int, **kwargs) -> int:\n",
        "    max_reward_arms = np.where(expected_rewards == max(expected_rewards))[0]\n",
        "    return np.random.choice(max_reward_arms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_nYx2CcZ3f6"
      },
      "source": [
        "# Run experiment with policy\n",
        "experiment = BanditExperiment(multi_armed_bandit, optimistic_initial_values, init_rewards=optimistic_initial_reward)\n",
        "experiment.run(num_steps=NUM_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wt3kyGjXZ3f6"
      },
      "source": [
        "# Visualise results\n",
        "experiment.visualise_results()\n",
        "\n",
        "md(f'''\n",
        "Optimal arm found with **OIV** is **{experiment.num_draws.argmax()}**\n",
        "\n",
        "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
        "\n",
        "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cQCpTbYZ3f6"
      },
      "source": [
        "Now let's compare the policy performance for different values of `init_rewards`, averaged over **200** separate experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FRSqyZfXZ3f7"
      },
      "source": [
        "average_runs(optimistic_initial_values, timesteps=NUM_STEPS, repeats=NUM_EXPERIMENTS, init_rewards=[1, 5, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynm2qrZsZ3f7"
      },
      "source": [
        "### Upper Confidence Bound (**UCB**)\n",
        "In this policy, we use the number of times each arm has been drawn to compute a confidence interval for the expected reward for each arm. At each step we select the arm with the highest upper bound for the confidence interval. The **weight given to uncertainty term**, $c \\in (0, 1]$ is a hyperparameter in this case.\n",
        "\n",
        "This method is the most popular policy used for bandits. The uncertainty bounds aid is exploration in the initial stages, but stops it as soon as soon as the optimal arm is found.\n",
        "\n",
        "One thing to keep in mind is UCB can only be applied after each arm has been tried at least once. This is to ensure that uncertainty for each arm can be computed. For this we add another hyperparameter for the **number of tries**. Until each arm has been pulled this minimum number of times, we randomly select one of the arms that has not yet been pulled enough times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW2YJf7sZ3f7"
      },
      "source": [
        "# PARAMETERS to play around with\n",
        "WEIGHT = 0.5\n",
        "NUM_TRIES_PER_ARM = 10\n",
        "\n",
        "def upper_confidence_bound(expected_rewards: List[float],\n",
        "                        num_draws: List[int],\n",
        "                        weight: float=WEIGHT,\n",
        "                        num_tries_per_arm: int=NUM_TRIES_PER_ARM) -> int:\n",
        "    # IMPLEMENT POLICY HERE\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8bqTu8cZ3f7"
      },
      "source": [
        "# @title SOLUTION for UCB policy\n",
        "def upper_confidence_bound(expected_rewards: List[float],\n",
        "                        num_draws: List[int],\n",
        "                        weight: float=WEIGHT,\n",
        "                        num_tries_per_arm: int=NUM_TRIES_PER_ARM) -> int:\n",
        "    if any(num_draws < num_tries_per_arm):\n",
        "        unpulled_arms = np.where(num_draws < num_tries_per_arm)[0]\n",
        "        return np.random.choice(unpulled_arms)\n",
        "\n",
        "    total_draws = sum(num_draws)\n",
        "    num_arms = len(expected_rewards)\n",
        "    uncertainty = np.sqrt(np.log(total_draws + num_arms) / (num_draws + num_arms))\n",
        "    ucb = expected_rewards + weight * uncertainty\n",
        "\n",
        "    max_reward_arms = np.where(ucb == max(ucb))[0]\n",
        "    return np.random.choice(max_reward_arms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz7HrcMHZ3f7"
      },
      "source": [
        "# Run experiment with policy\n",
        "experiment = BanditExperiment(multi_armed_bandit, upper_confidence_bound)\n",
        "experiment.run(num_steps=NUM_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbp2FVnpZ3f7"
      },
      "source": [
        "# Visualise results\n",
        "experiment.visualise_results()\n",
        "\n",
        "md(f'''\n",
        "Optimal arm found with **UCB** is **{experiment.num_draws.argmax()}**\n",
        "\n",
        "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
        "\n",
        "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6wpLfBtZ3f7"
      },
      "source": [
        "Now let's compare the policy performance for different values of `num_tries_per_arm`, averaged over **200** separate experiments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFFHd9vCZ3f8"
      },
      "source": [
        "average_runs(upper_confidence_bound, timesteps=NUM_STEPS, repeats=NUM_EXPERIMENTS, weight=0.5, num_tries_per_arm=[1, 2, 10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbBze71GZ3f8"
      },
      "source": [
        "## OPTIONAL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NmX4X_RnZ3f8"
      },
      "source": [
        "### Thompson Sampling\n",
        "In this policy, we are going to put a **prior** on each arm and learn the distribution of the rewards for each pulled arm directly.\n",
        "\n",
        "To keep things simple, we will look at a multi-armed bandit where the reward for each arm comes from a Bernoulli distribution. We will define the prior over each arm with a Beta distribution.\n",
        "\n",
        "To that end, we will make a few modifications to our setup. We will start with redifining our mean sampling functions and using them to create a Bernoulli testbed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0v0Lt8tZ3f8"
      },
      "source": [
        "def uniform_mean_sampler(size: int=1) -> float:\n",
        "    return np.random.uniform(size=size)\n",
        "\n",
        "def bernoulli_reward_sampler(p: float, size: int=1) -> float:\n",
        "    reward = np.random.binomial(n=1, p=p, size=size)\n",
        "    if size == 1:\n",
        "        return reward[0]\n",
        "    return reward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coL8XFAxZ3f8"
      },
      "source": [
        "bayesian_bandit = MultiArmedBandit(num_arms=NUM_ARMS, arm_mean_sampler=uniform_mean_sampler, arm_reward_sampler=bernoulli_reward_sampler)\n",
        "sns.scatterplot(x=range(NUM_ARMS), y=bayesian_bandit.mean_rewards, ax=plt.subplots(figsize=(15, 5))[1])\n",
        "plt.ylim(0, 1)\n",
        "plt.xlabel('Arm')\n",
        "plt.ylabel('Mean reward')\n",
        "plt.show()\n",
        "\n",
        "md(f\"Arm with **maximum mean reward** is arm **{bayesian_bandit.mean_rewards.argmax()}**\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R0csITwrZ3f8"
      },
      "source": [
        "We will also redefine how our experiments are conducted with a Beta prior for a MultiArmedBandit with Bernoulli rewards. For the curious, the code is provided in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sw_t_AQtZ3f8"
      },
      "source": [
        "# @title Experiment code for Thompson sampling experiment\n",
        "class ThompsonExperiment(BanditExperiment):\n",
        "    \n",
        "    def _init_fields(self) -> None:\n",
        "        self.observed_rewards = []\n",
        "        # alphas - first column, betas - second column\n",
        "        self.beta_params = np.tile([alpha, beta], self.multi_armed_bandit.num_arms).reshape(self.multi_armed_bandit.num_arms, 2)\n",
        "        self.observed_draws = []\n",
        "        self.num_draws = np.zeros(self.multi_armed_bandit.num_arms, dtype=np.int32)\n",
        "        self.total_draws = 0\n",
        "\n",
        "    def step(self) -> Tuple[int, float]:\n",
        "        chosen_arm = self.policy(self.beta_params)\n",
        "        reward = self.multi_armed_bandit.draw_arm(chosen_arm)\n",
        "\n",
        "        self.total_draws += 1\n",
        "        self.observed_draws.append(chosen_arm)\n",
        "        self.num_draws[chosen_arm] += 1\n",
        "        self.observed_rewards.append(reward)\n",
        "        if reward == 1:\n",
        "            self.beta_params[chosen_arm, 0] += 1\n",
        "        else:\n",
        "            self.beta_params[chosen_arm, 1] += 1\n",
        "\n",
        "        return chosen_arm, reward\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhoIi8u-Z3f9"
      },
      "source": [
        "Now let's implement the sampling policy. For Thompson sampling, we will use the prior parameters to sample expected rewards for each arm and select the arm with the highest sampled reward. Depending on the observed reward, we update the prior parameters for the respective arm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "01eLoVSzZ3f9"
      },
      "source": [
        "# PARAMETERS set initial Beta prior parameters\n",
        "alpha = 0\n",
        "beta = 0\n",
        "\n",
        "def beta_bernoulli_priors(beta_params: List[List[int]]) -> int:\n",
        "    # IMPLEMENT POLICY HERE\n",
        "    pass\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXRgINGVZ3f9"
      },
      "source": [
        "# @title SOLUTION for Beta-Bernoulli-priors policy\n",
        "def beta_bernoulli_priors(beta_params: List[List[int]]) -> int:\n",
        "    sampled_rewards = []\n",
        "    for i in range(len(beta_params)):\n",
        "        alpha, beta = beta_params[i]\n",
        "        reward = scipy.stats.beta.rvs(alpha + 1, beta + 1)\n",
        "        sampled_rewards.append(reward)\n",
        "\n",
        "    max_reward_arms = np.where(np.array(sampled_rewards) == max(sampled_rewards))[0]\n",
        "    return np.random.choice(max_reward_arms)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0ooo8BtZ3f9"
      },
      "source": [
        "# run experiment with policy\n",
        "experiment = ThompsonExperiment(bayesian_bandit, beta_bernoulli_priors)\n",
        "experiment.run(num_steps=NUM_STEPS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6BN_qEBLZ3f9"
      },
      "source": [
        "experiment.visualise_results(kind='scatter')\n",
        "\n",
        "md(f'''\n",
        "Optimal arm found with **Thompson sampling** is **{experiment.num_draws.argmax()}**\n",
        "\n",
        "Number of times each arm was **pulled**: {dict(zip(range(len(experiment.num_draws)), experiment.num_draws))}\n",
        "\n",
        "**Average reward** collected is **{np.array(experiment.observed_rewards).mean()}**\n",
        "''')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}